{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "mathematical-mortality",
   "metadata": {},
   "outputs": [],
   "source": [
    "from facenet_pytorch import InceptionResnetV1, fixed_image_standardization\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from PIL import Image\n",
    "import random\n",
    "import torch\n",
    "\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import transforms\n",
    "from torchvision import models as models\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "from torch.backends import cudnn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.compose import ColumnTransformer, make_column_transformer, make_column_selector\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.stats import sem\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import albumentations as A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "every-river",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_dataset/4.png\n",
      "test_dataset/5.png\n",
      "test_dataset/7.jpg\n",
      "test_dataset/6.png\n",
      "test_dataset/2.png\n",
      "test_dataset/3.png\n",
      "test_dataset/1.png\n"
     ]
    }
   ],
   "source": [
    "# преобразовать в соответствии с логикой\n",
    "test_images_path = 'test_dataset/'\n",
    "\n",
    "def load_images_from_folder(folder): \n",
    "    images = []\n",
    "    for filename in os.listdir(folder): # читаю все предобратанные картинки с фигурами\n",
    "        print(os.path.join(folder,filename))\n",
    "        images.append(os.path.join(folder,filename))\n",
    "    return images\n",
    "\n",
    "images = load_images_from_folder(test_images_path)\n",
    "#images = pd.DataFrame(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "related-prophet",
   "metadata": {},
   "outputs": [],
   "source": [
    "genderModel = InceptionResnetV1(\n",
    "    classify=True,\n",
    "    pretrained='vggface2',\n",
    "    num_classes= 2)\n",
    "\n",
    "genderModel.load_state_dict(torch.load('model0.pth'))\n",
    "genderModel.eval()\n",
    "\n",
    "#ageModel = InceptionResnetV1(\n",
    "#    classify=True,\n",
    "#    pretrained='vggface2',\n",
    "#    num_classes= 2)\n",
    "#\n",
    "#ageModel.load_state_dict(torch.load('model0.pth'))\n",
    "#ageModel.eval()\n",
    "\n",
    "trans = transforms.Compose([\n",
    "    np.float32,\n",
    "    transforms.ToTensor(),\n",
    "    fixed_image_standardization\n",
    "])\n",
    "\n",
    "class GenderDataset(Dataset):\n",
    "    def __init__(self, path, image_files, labels_age, labels_gender, p_augment=0.5,  validation=False):\n",
    "        self.path = path\n",
    "        self.X = image_files\n",
    "        self.y_age = labels_age\n",
    "        self.y_gender = labels_gender\n",
    "        self.resize = A.Resize(160, 160, always_apply=True)\n",
    "        self.transform = trans\n",
    "        \n",
    "    def __len__(self):\n",
    "        return (len(self.X))\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        image = Image.open(self.path + self.X[i])\n",
    "        image = image.convert('RGB')\n",
    "        image = np.asarray(image)\n",
    "        image = self.resize(image=image)['image']\n",
    "        image = self.transform(image)\n",
    "        label_age = self.y_age[i]\n",
    "        label_gender = self.y_gender[i]\n",
    "        \n",
    "        return torch.tensor(image, dtype=torch.float), torch.tensor(label_gender, dtype=torch.long)\n",
    "\n",
    "def predict_by_image(image_path):\n",
    "    device = torch.device(\"cpu\")\n",
    "    results = torch.empty(0).to(device)\n",
    "\n",
    "    dataset = GenderDataset('',\n",
    "                            [image_path], \n",
    "                            [0],\n",
    "                            [0], \n",
    "                            validation=True)\n",
    "\n",
    "    loader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "    for batch in loader:\n",
    "        batch_data, batch_label = batch\n",
    "        predict = genderModel(batch_data)\n",
    "\n",
    "        results = torch.cat((results,\n",
    "                             nn.functional.softmax(predict.detach(),dim=1)), 0)\n",
    "\n",
    "        results = results.cpu().numpy()\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fuzzy-subsection",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_dataset/3.png [[0.99742657 0.00257345]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-ac2e1cac41bc>:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(image, dtype=torch.float), torch.tensor(label_gender, dtype=torch.long)\n"
     ]
    }
   ],
   "source": [
    "predict = predict_by_image(images[5])\n",
    "print(images[5], predict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

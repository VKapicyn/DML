{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "included-mumbai",
   "metadata": {},
   "outputs": [],
   "source": [
    "from facenet_pytorch import InceptionResnetV1, fixed_image_standardization\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from PIL import Image\n",
    "import random\n",
    "import torch\n",
    "\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import transforms\n",
    "from torchvision import models as models\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "from torch.backends import cudnn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.compose import ColumnTransformer, make_column_transformer, make_column_selector\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.stats import sem\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import albumentations as A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dedicated-artist",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = 'dataset/'\n",
    "image_path = dataset_path+'faces/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "everyday-perception",
   "metadata": {},
   "outputs": [],
   "source": [
    "# добавляем рандом\n",
    "def seed_everything(seed):\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    #torch.cuda.manual_seed(seed)\n",
    "\n",
    "random_seed = 1120\n",
    "seed_everything(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "emerging-collapse",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Преобразуем возрастные лейблы в 8 категорий\n",
    "group_8class_age ={\n",
    "    '0-2':['(0, 2)','2'],\n",
    "    '4-6':['(4, 6)'],\n",
    "    '8-13':['(8, 12)','13'],\n",
    "    '15-20':['(15, 20)'],\n",
    "    '25-32':['(25, 32)','(27, 32)','32','29'],\n",
    "    '38-43':['(38, 43)','35', '36', '42','(38, 42)'],\n",
    "    '48-53':['(48, 53)','42'],\n",
    "    '60-':['(60, 100)']\n",
    "}\n",
    "\n",
    "# Алтернативное группирование, по 4ем категориям\n",
    "group_4class_age={\n",
    "'Личинус (0-6)':    ['(0, 2)', '2', '(4, 6)', '3'],\n",
    "'Щегол (8-23)': ['(8, 12)', '(15, 20)', '(8, 23)', '23', '22', '13'],\n",
    "'Бумер (25-32)':  ['(25, 32)', '(27, 32)', '32', '34', '29'],\n",
    "'Старпёр 35+':  ['(48, 53)', '(60, 100)', '55','56','57', '58', '(38, 42)','(38, 43)', '(38, 48)', '35', '36', '46', '45', '42'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "failing-trout",
   "metadata": {},
   "outputs": [],
   "source": [
    "# формируем лейблы на основе категорий\n",
    "def map_age(group_to_age):\n",
    "    age_to_group = {}\n",
    "    for group in group_to_age.keys():\n",
    "        age = group_to_age[group]\n",
    "        for aa in age:\n",
    "            age_to_group[aa] = group\n",
    "    return age_to_group\n",
    "\n",
    "# берем группировку из 4ех категорий\n",
    "age_grouping = group_4class_age\n",
    "age_to_label = map_age(age_grouping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "flush-victim",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "third-breach",
   "metadata": {},
   "outputs": [],
   "source": [
    "#считываем размеченный данные\n",
    "test_fold = [pd.read_csv(f\"{dataset_path}fold_0_data.txt\",sep = \"\\t\")]\n",
    "test_fold.append(pd.read_csv(f\"{dataset_path}fold_1_data.txt\",sep = \"\\t\"))\n",
    "test_fold.append(pd.read_csv(f\"{dataset_path}fold_2_data.txt\",sep = \"\\t\"))\n",
    "test_fold.append(pd.read_csv(f\"{dataset_path}fold_3_data.txt\",sep = \"\\t\"))\n",
    "test_fold.append(pd.read_csv(f\"{dataset_path}fold_4_data.txt\",sep = \"\\t\"))\n",
    "\n",
    "all_age_group = pd.concat([test_fold[0],test_fold[1],test_fold[2], test_fold[3],test_fold[4]]).age.value_counts()\n",
    "all_age_group = list(all_age_group.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fancy-modern",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1  : (3995, 3)\n",
      "\n",
      "Fold 2  : (3597, 3)\n",
      "\n",
      "Fold 3  : (3124, 3)\n",
      "\n",
      "Fold 4  : (3291, 3)\n",
      "\n",
      "Fold 5  : (3445, 3)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "drop_columns=['user_id', 'original_image', 'face_id', 'x', 'y', 'dx',\n",
    "       'dy', 'tilt_ang', 'fiducial_yaw_angle', 'fiducial_score']\n",
    "\n",
    "for fold, df in enumerate(test_fold):\n",
    "    \n",
    "    #полный путь до изображения\n",
    "    df['image_path'] = image_path + df['user_id'] + '/coarse_tilt_aligned_face.' + \\\n",
    "        df['face_id'].astype('str') + '.' + df['original_image']\n",
    "   \n",
    "    #удаляем лишние колонки\n",
    "    df.drop(drop_columns, axis=1, inplace=True)\n",
    "    \n",
    "    df.gender = df.gender.astype(str)\n",
    "    df.age = df.age.astype(str)\n",
    "    \n",
    "    #удаляем строки с пустыми значениями\n",
    "    df.drop(df[df.gender == 'u'].index, inplace=True)\n",
    "    df.drop(df[df.gender == 'nan'].index, inplace=True)\n",
    "    df.drop(df[df.age == 'None'].index, inplace=True)\n",
    "    \n",
    "    #лейбл возраста -> лейбл возрастной категории\n",
    "    include_age = list(age_to_label.keys())\n",
    "    exclude_age = list(set(all_age_group) - set(include_age))\n",
    "    \n",
    "    #удаляем если вдруг не попало ни в одну возрастную категорию\n",
    "    for exc_age in exclude_age:\n",
    "        df.drop(df.loc[df['age']==exc_age].index, inplace=True)\n",
    "        \n",
    "    #упорядочиваем индексы по возрасту\n",
    "    df['age'] = df['age'].apply(lambda x: age_to_label[x])\n",
    "    \n",
    "    print(f'Fold {fold+1}  : {df.shape}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "designing-tender",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1  : (13457, 3)\n",
      "Fold 2  : (13855, 3)\n",
      "Fold 3  : (14328, 3)\n",
      "Fold 4  : (14161, 3)\n",
      "Fold 5  : (14007, 3)\n"
     ]
    }
   ],
   "source": [
    "# 5 тренировочных фолдов, в виде разной последовтальности тестовых? кажется над переделать\n",
    "train_fold = [pd.concat([test_fold[1],test_fold[2],test_fold[3],test_fold[4]],ignore_index=True)]\n",
    "train_fold.append(pd.concat([test_fold[0],test_fold[2],test_fold[3],test_fold[4]],ignore_index=True))\n",
    "train_fold.append(pd.concat([test_fold[0],test_fold[1],test_fold[3],test_fold[4]],ignore_index=True))\n",
    "train_fold.append(pd.concat([test_fold[0],test_fold[1],test_fold[2],test_fold[4]],ignore_index=True))\n",
    "train_fold.append(pd.concat([test_fold[0],test_fold[1],test_fold[2],test_fold[3]],ignore_index=True))\n",
    "\n",
    "# датафрейм со всем фолдами\n",
    "all_fold = pd.concat([test_fold[0],test_fold[1],test_fold[2],test_fold[3],test_fold[4]],ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "literary-familiar",
   "metadata": {},
   "outputs": [],
   "source": [
    "# приводим значения к числовым\n",
    "gender_to_label_map = {\n",
    "    'f' : 0,\n",
    "    'm' : 1\n",
    "}\n",
    "\n",
    "# приводим значения к числовым\n",
    "age_to_label_map = {\n",
    "    'Личинус (0-6)' :0,\n",
    "    'Щегол (8-23)' :1,\n",
    "    'Бумер (25-32)' :2,\n",
    "    'Старпёр 35+' :3\n",
    "}\n",
    "\n",
    "label_to_age_map = {value: key for key, value in age_to_label_map.items()}\n",
    "label_to_gender_map = {value: key for key, value in gender_to_label_map.items()}\n",
    "\n",
    "all_fold['age'].replace(age_to_label_map, inplace=True)\n",
    "all_fold['gender'].replace(gender_to_label_map, inplace=True)\n",
    "\n",
    "for i, fold in enumerate(train_fold):\n",
    "    fold['age'].replace(age_to_label_map, inplace=True)\n",
    "    fold['gender'].replace(gender_to_label_map, inplace=True)\n",
    "\n",
    "for i, fold in enumerate(test_fold):\n",
    "    fold['age'].replace(age_to_label_map, inplace=True)\n",
    "    fold['gender'].replace(gender_to_label_map, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "divine-bride",
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "### Классы моделей + аугментация (рамки)\n",
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "auburn-modern",
   "metadata": {},
   "outputs": [],
   "source": [
    "trans = transforms.Compose([\n",
    "    np.float32,\n",
    "    transforms.ToTensor(),\n",
    "    fixed_image_standardization\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "reasonable-software",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgeDataset(Dataset):\n",
    "    def __init__(self, path, image_files, labels_age, labels_gender, p_augment=0.5,  validation=False):\n",
    "        self.path = path\n",
    "        self.X = image_files\n",
    "        self.y_age = labels_age\n",
    "        self.y_gender = labels_gender\n",
    "        self.resize = A.Resize(160, 160, always_apply=True)\n",
    "        self.transform = trans\n",
    "        \n",
    "    def __len__(self):\n",
    "        return (len(self.X))\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        image = Image.open(self.path + self.X[i])\n",
    "        image = np.asarray(image)\n",
    "        image = self.resize(image=image)['image']\n",
    "        image = self.transform(image)\n",
    "        label_age = self.y_age[i]\n",
    "        label_gender = self.y_gender[i]\n",
    "        \n",
    "        return torch.tensor(image, dtype=torch.float), torch.tensor(label_age, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "governing-relation",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenderDataset(Dataset):\n",
    "    def __init__(self, path, image_files, labels_age, labels_gender, p_augment=0.5,  validation=False):\n",
    "        self.path = path\n",
    "        self.X = image_files\n",
    "        self.y_age = labels_age\n",
    "        self.y_gender = labels_gender\n",
    "        self.resize = A.Resize(160, 160, always_apply=True)\n",
    "        self.transform = trans\n",
    "        \n",
    "    def __len__(self):\n",
    "        return (len(self.X))\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        image = Image.open(self.path + self.X[i])\n",
    "        image = np.asarray(image)\n",
    "        image = self.resize(image=image)['image']\n",
    "        image = self.transform(image)\n",
    "        label_age = self.y_age[i]\n",
    "        label_gender = self.y_gender[i]\n",
    "        \n",
    "        return torch.tensor(image, dtype=torch.float), torch.tensor(label_gender, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "favorite-handle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data: 12111\n",
      "Val. data: 1346\n",
      "Training data: 12469\n",
      "Val. data: 1386\n",
      "Training data: 12895\n",
      "Val. data: 1433\n",
      "Training data: 12744\n",
      "Val. data: 1417\n",
      "Training data: 12606\n",
      "Val. data: 1401\n"
     ]
    }
   ],
   "source": [
    "train_split={}\n",
    "val_split={}\n",
    "\n",
    "for fold in range(5):\n",
    "    #create train-validation stratified split\n",
    "    sss = StratifiedShuffleSplit(n_splits=10, random_state=random_seed)\n",
    "    \n",
    "    train_data = train_fold[fold]['image_path'].copy().reset_index(drop=True).to_list()\n",
    "    train_gender_label = train_fold[fold]['gender'].copy().reset_index(drop=True).to_list()\n",
    "    train_age_label = train_fold[fold]['age'].copy().reset_index(drop=True).to_list()\n",
    "    train_idx, val_idx = list(sss.split(train_data, train_age_label))[0]\n",
    "    \n",
    "    print(f'Training data: {len(train_idx)}')\n",
    "    print(f'Val. data: {len(val_idx)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "colored-allocation",
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "### Гиперпараметры (!!! говнокод. Переделать!!!)\n",
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "necessary-toronto",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 2)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print_sample=False\n",
    "kfold = 1\n",
    "batchsize = 64\n",
    "lr_age = 3e-5\n",
    "lr_gender= 2e-5\n",
    "num_epochs = 10\n",
    "p_augment = 0.0\n",
    "augment=False\n",
    "\n",
    "\n",
    "#General parameters\n",
    "to_predict = 'gender'\n",
    "device = torch.device(\"cpu\")\n",
    "num_age_classes = train_fold[0].age.value_counts().shape[0]\n",
    "num_gender_classes = train_fold[0].gender.value_counts().shape[0]\n",
    "num_age_classes, num_gender_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "crucial-rental",
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "### Обучение\n",
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "greatest-wagner",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- Классифиуируем только возраст  ----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "swedish-brick",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "\n",
      "epoch: 0\n",
      "\n",
      "trained: 0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-15-c5aa7d387409>:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(image, dtype=torch.float), torch.tensor(label_gender, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_num: 0.0%\n",
      "\n",
      "batch_num: 0.5263157894736842%\n",
      "\n",
      "batch_num: 1.0526315789473684%\n",
      "\n",
      "batch_num: 1.5789473684210527%\n",
      "\n",
      "batch_num: 2.1052631578947367%\n",
      "\n",
      "batch_num: 2.631578947368421%\n",
      "\n",
      "batch_num: 3.1578947368421053%\n",
      "\n",
      "batch_num: 3.684210526315789%\n",
      "\n",
      "batch_num: 4.2105263157894735%\n",
      "\n",
      "batch_num: 4.736842105263158%\n",
      "\n",
      "batch_num: 5.263157894736842%\n",
      "\n",
      "batch_num: 5.7894736842105265%\n",
      "\n",
      "batch_num: 6.315789473684211%\n",
      "\n",
      "batch_num: 6.842105263157896%\n",
      "\n",
      "batch_num: 7.368421052631578%\n",
      "\n",
      "batch_num: 7.894736842105263%\n",
      "\n",
      "batch_num: 8.421052631578947%\n",
      "\n",
      "batch_num: 8.947368421052632%\n",
      "\n",
      "batch_num: 9.473684210526317%\n",
      "\n",
      "batch_num: 10.0%\n",
      "\n",
      "batch_num: 10.526315789473683%\n",
      "\n",
      "batch_num: 11.052631578947368%\n",
      "\n",
      "batch_num: 11.578947368421053%\n",
      "\n",
      "batch_num: 12.105263157894736%\n",
      "\n",
      "batch_num: 12.631578947368421%\n",
      "\n",
      "batch_num: 13.157894736842104%\n",
      "\n",
      "batch_num: 13.684210526315791%\n",
      "\n",
      "batch_num: 14.210526315789473%\n",
      "\n",
      "batch_num: 14.736842105263156%\n",
      "\n",
      "batch_num: 15.263157894736842%\n",
      "\n",
      "batch_num: 15.789473684210526%\n",
      "\n",
      "batch_num: 16.315789473684212%\n",
      "\n",
      "batch_num: 16.842105263157894%\n",
      "\n",
      "batch_num: 17.36842105263158%\n",
      "\n",
      "batch_num: 17.894736842105264%\n",
      "\n",
      "batch_num: 18.421052631578945%\n",
      "\n",
      "batch_num: 18.947368421052634%\n",
      "\n",
      "batch_num: 19.473684210526315%\n",
      "\n",
      "batch_num: 20.0%\n",
      "\n",
      "batch_num: 20.526315789473685%\n",
      "\n",
      "batch_num: 21.052631578947366%\n",
      "\n",
      "batch_num: 21.578947368421055%\n",
      "\n",
      "batch_num: 22.105263157894736%\n",
      "\n",
      "batch_num: 22.63157894736842%\n",
      "\n",
      "batch_num: 23.157894736842106%\n",
      "\n",
      "batch_num: 23.684210526315788%\n",
      "\n",
      "batch_num: 24.210526315789473%\n",
      "\n",
      "batch_num: 24.736842105263158%\n",
      "\n",
      "batch_num: 25.263157894736842%\n",
      "\n",
      "batch_num: 25.789473684210527%\n",
      "\n",
      "batch_num: 26.31578947368421%\n",
      "\n",
      "batch_num: 26.842105263157894%\n",
      "\n",
      "batch_num: 27.368421052631582%\n",
      "\n",
      "batch_num: 27.89473684210526%\n",
      "\n",
      "batch_num: 28.421052631578945%\n",
      "\n",
      "batch_num: 28.947368421052634%\n",
      "\n",
      "batch_num: 29.47368421052631%\n",
      "\n",
      "batch_num: 30.0%\n",
      "\n",
      "batch_num: 30.526315789473685%\n",
      "\n",
      "batch_num: 31.05263157894737%\n",
      "\n",
      "batch_num: 31.57894736842105%\n",
      "\n",
      "batch_num: 32.10526315789474%\n",
      "\n",
      "batch_num: 32.631578947368425%\n",
      "\n",
      "batch_num: 33.1578947368421%\n",
      "\n",
      "batch_num: 33.68421052631579%\n",
      "\n",
      "batch_num: 34.21052631578947%\n",
      "\n",
      "batch_num: 34.73684210526316%\n",
      "\n",
      "batch_num: 35.26315789473684%\n",
      "\n",
      "batch_num: 35.78947368421053%\n",
      "\n",
      "batch_num: 36.31578947368421%\n",
      "\n",
      "batch_num: 36.84210526315789%\n",
      "\n",
      "batch_num: 37.368421052631575%\n",
      "\n",
      "batch_num: 37.89473684210527%\n",
      "\n",
      "batch_num: 38.421052631578945%\n",
      "\n",
      "batch_num: 38.94736842105263%\n",
      "\n",
      "batch_num: 39.473684210526315%\n",
      "\n",
      "batch_num: 40.0%\n",
      "\n",
      "batch_num: 40.526315789473685%\n",
      "\n",
      "batch_num: 41.05263157894737%\n",
      "\n",
      "batch_num: 41.578947368421055%\n",
      "\n",
      "batch_num: 42.10526315789473%\n",
      "\n",
      "batch_num: 42.63157894736842%\n",
      "\n",
      "batch_num: 43.15789473684211%\n",
      "\n",
      "batch_num: 43.684210526315795%\n",
      "\n",
      "batch_num: 44.21052631578947%\n",
      "\n",
      "batch_num: 44.73684210526316%\n",
      "\n",
      "batch_num: 45.26315789473684%\n",
      "\n",
      "batch_num: 45.78947368421053%\n",
      "\n",
      "batch_num: 46.31578947368421%\n",
      "\n",
      "batch_num: 46.8421052631579%\n",
      "\n",
      "batch_num: 47.368421052631575%\n",
      "\n",
      "batch_num: 47.89473684210526%\n",
      "\n",
      "batch_num: 48.421052631578945%\n",
      "\n",
      "batch_num: 48.94736842105264%\n",
      "\n",
      "batch_num: 49.473684210526315%\n",
      "\n",
      "batch_num: 50.0%\n",
      "\n",
      "batch_num: 50.526315789473685%\n",
      "\n",
      "batch_num: 51.05263157894737%\n",
      "\n",
      "batch_num: 51.578947368421055%\n",
      "\n",
      "batch_num: 52.10526315789473%\n",
      "\n",
      "batch_num: 52.63157894736842%\n",
      "\n",
      "batch_num: 53.1578947368421%\n",
      "\n",
      "batch_num: 53.68421052631579%\n",
      "\n",
      "batch_num: 54.21052631578947%\n",
      "\n",
      "batch_num: 54.736842105263165%\n",
      "\n",
      "batch_num: 55.26315789473685%\n",
      "\n",
      "batch_num: 55.78947368421052%\n",
      "\n",
      "batch_num: 56.315789473684205%\n",
      "\n",
      "batch_num: 56.84210526315789%\n",
      "\n",
      "batch_num: 57.36842105263158%\n",
      "\n",
      "batch_num: 57.89473684210527%\n",
      "\n",
      "batch_num: 58.42105263157895%\n",
      "\n",
      "batch_num: 58.94736842105262%\n",
      "\n",
      "batch_num: 59.473684210526315%\n",
      "\n",
      "batch_num: 60.0%\n",
      "\n",
      "batch_num: 60.526315789473685%\n",
      "\n",
      "batch_num: 61.05263157894737%\n",
      "\n",
      "batch_num: 61.578947368421055%\n",
      "\n",
      "batch_num: 62.10526315789474%\n",
      "\n",
      "batch_num: 62.63157894736842%\n",
      "\n",
      "batch_num: 63.1578947368421%\n",
      "\n",
      "batch_num: 63.68421052631579%\n",
      "\n",
      "batch_num: 64.21052631578948%\n",
      "\n",
      "batch_num: 64.73684210526316%\n",
      "\n",
      "batch_num: 65.26315789473685%\n",
      "\n",
      "batch_num: 65.78947368421053%\n",
      "\n",
      "batch_num: 66.3157894736842%\n",
      "\n",
      "batch_num: 66.84210526315789%\n",
      "\n",
      "batch_num: 67.36842105263158%\n",
      "\n",
      "batch_num: 67.89473684210526%\n",
      "\n",
      "batch_num: 68.42105263157895%\n",
      "\n",
      "batch_num: 68.94736842105263%\n",
      "\n",
      "batch_num: 69.47368421052632%\n",
      "\n",
      "batch_num: 70.0%\n",
      "\n",
      "batch_num: 70.52631578947368%\n",
      "\n",
      "batch_num: 71.05263157894737%\n",
      "\n",
      "batch_num: 71.57894736842105%\n",
      "\n",
      "batch_num: 72.10526315789474%\n",
      "\n",
      "batch_num: 72.63157894736842%\n",
      "\n",
      "batch_num: 73.15789473684211%\n",
      "\n",
      "batch_num: 73.68421052631578%\n",
      "\n",
      "batch_num: 74.21052631578947%\n",
      "\n",
      "batch_num: 74.73684210526315%\n",
      "\n",
      "batch_num: 75.26315789473685%\n",
      "\n",
      "batch_num: 75.78947368421053%\n",
      "\n",
      "batch_num: 76.31578947368422%\n",
      "\n",
      "batch_num: 76.84210526315789%\n",
      "\n",
      "batch_num: 77.36842105263158%\n",
      "\n",
      "batch_num: 77.89473684210526%\n",
      "\n",
      "batch_num: 78.42105263157895%\n",
      "\n",
      "batch_num: 78.94736842105263%\n",
      "\n",
      "batch_num: 79.47368421052632%\n",
      "\n",
      "batch_num: 80.0%\n",
      "\n",
      "batch_num: 80.52631578947368%\n",
      "\n",
      "batch_num: 81.05263157894737%\n",
      "\n",
      "batch_num: 81.57894736842105%\n",
      "\n",
      "batch_num: 82.10526315789474%\n",
      "\n",
      "batch_num: 82.63157894736842%\n",
      "\n",
      "batch_num: 83.15789473684211%\n",
      "\n",
      "batch_num: 83.6842105263158%\n",
      "\n",
      "batch_num: 84.21052631578947%\n",
      "\n",
      "batch_num: 84.73684210526315%\n",
      "\n",
      "batch_num: 85.26315789473684%\n",
      "\n",
      "batch_num: 85.78947368421052%\n",
      "\n",
      "batch_num: 86.31578947368422%\n",
      "\n",
      "batch_num: 86.8421052631579%\n",
      "\n",
      "batch_num: 87.36842105263159%\n",
      "\n",
      "batch_num: 87.89473684210526%\n",
      "\n",
      "batch_num: 88.42105263157895%\n",
      "\n",
      "batch_num: 88.94736842105263%\n",
      "\n",
      "batch_num: 89.47368421052632%\n",
      "\n",
      "batch_num: 90.0%\n",
      "\n",
      "batch_num: 90.52631578947368%\n",
      "\n",
      "batch_num: 91.05263157894737%\n",
      "\n",
      "batch_num: 91.57894736842105%\n",
      "\n",
      "batch_num: 92.10526315789474%\n",
      "\n",
      "batch_num: 92.63157894736842%\n",
      "\n",
      "batch_num: 93.15789473684211%\n",
      "\n",
      "batch_num: 93.6842105263158%\n",
      "\n",
      "batch_num: 94.21052631578948%\n",
      "\n",
      "batch_num: 94.73684210526315%\n",
      "\n",
      "batch_num: 95.26315789473684%\n",
      "\n",
      "batch_num: 95.78947368421052%\n",
      "\n",
      "batch_num: 96.3157894736842%\n",
      "\n",
      "batch_num: 96.84210526315789%\n",
      "\n",
      "batch_num: 97.36842105263158%\n",
      "\n",
      "batch_num: 97.89473684210527%\n",
      "\n",
      "batch_num: 98.42105263157895%\n",
      "\n",
      "batch_num: 98.94736842105263%\n",
      "\n",
      "batch_num: 99.47368421052632%\n",
      "\n",
      "Epoch 0 | train loss: 0.18190792947027243 | val loss: 0.07678648994558236 | accuracy: 97.55%\n",
      "epoch: 1\n",
      "\n",
      "trained: 1\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-15-c5aa7d387409>:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(image, dtype=torch.float), torch.tensor(label_gender, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_num: 0.0%\n",
      "\n",
      "batch_num: 0.5263157894736842%\n",
      "\n",
      "batch_num: 1.0526315789473684%\n",
      "\n",
      "batch_num: 1.5789473684210527%\n",
      "\n",
      "batch_num: 2.1052631578947367%\n",
      "\n",
      "batch_num: 2.631578947368421%\n",
      "\n",
      "batch_num: 3.1578947368421053%\n",
      "\n",
      "batch_num: 3.684210526315789%\n",
      "\n",
      "batch_num: 4.2105263157894735%\n",
      "\n",
      "batch_num: 4.736842105263158%\n",
      "\n",
      "batch_num: 5.263157894736842%\n",
      "\n",
      "batch_num: 5.7894736842105265%\n",
      "\n",
      "batch_num: 6.315789473684211%\n",
      "\n",
      "batch_num: 6.842105263157896%\n",
      "\n",
      "batch_num: 7.368421052631578%\n",
      "\n",
      "batch_num: 7.894736842105263%\n",
      "\n",
      "batch_num: 8.421052631578947%\n",
      "\n",
      "batch_num: 8.947368421052632%\n",
      "\n",
      "batch_num: 9.473684210526317%\n",
      "\n",
      "batch_num: 10.0%\n",
      "\n",
      "batch_num: 10.526315789473683%\n",
      "\n",
      "batch_num: 11.052631578947368%\n",
      "\n",
      "batch_num: 11.578947368421053%\n",
      "\n",
      "batch_num: 12.105263157894736%\n",
      "\n",
      "batch_num: 12.631578947368421%\n",
      "\n",
      "batch_num: 13.157894736842104%\n",
      "\n",
      "batch_num: 13.684210526315791%\n",
      "\n",
      "batch_num: 14.210526315789473%\n",
      "\n",
      "batch_num: 14.736842105263156%\n",
      "\n",
      "batch_num: 15.263157894736842%\n",
      "\n",
      "batch_num: 15.789473684210526%\n",
      "\n",
      "batch_num: 16.315789473684212%\n",
      "\n",
      "batch_num: 16.842105263157894%\n",
      "\n",
      "batch_num: 17.36842105263158%\n",
      "\n",
      "batch_num: 17.894736842105264%\n",
      "\n",
      "batch_num: 18.421052631578945%\n",
      "\n",
      "batch_num: 18.947368421052634%\n",
      "\n",
      "batch_num: 19.473684210526315%\n",
      "\n",
      "batch_num: 20.0%\n",
      "\n",
      "batch_num: 20.526315789473685%\n",
      "\n",
      "batch_num: 21.052631578947366%\n",
      "\n",
      "batch_num: 21.578947368421055%\n",
      "\n",
      "batch_num: 22.105263157894736%\n",
      "\n",
      "batch_num: 22.63157894736842%\n",
      "\n",
      "batch_num: 23.157894736842106%\n",
      "\n",
      "batch_num: 23.684210526315788%\n",
      "\n",
      "batch_num: 24.210526315789473%\n",
      "\n",
      "batch_num: 24.736842105263158%\n",
      "\n",
      "batch_num: 25.263157894736842%\n",
      "\n",
      "batch_num: 25.789473684210527%\n",
      "\n",
      "batch_num: 26.31578947368421%\n",
      "\n",
      "batch_num: 26.842105263157894%\n",
      "\n",
      "batch_num: 27.368421052631582%\n",
      "\n",
      "batch_num: 27.89473684210526%\n",
      "\n",
      "batch_num: 28.421052631578945%\n",
      "\n",
      "batch_num: 28.947368421052634%\n",
      "\n",
      "batch_num: 29.47368421052631%\n",
      "\n",
      "batch_num: 30.0%\n",
      "\n",
      "batch_num: 30.526315789473685%\n",
      "\n",
      "batch_num: 31.05263157894737%\n",
      "\n",
      "batch_num: 31.57894736842105%\n",
      "\n",
      "batch_num: 32.10526315789474%\n",
      "\n",
      "batch_num: 32.631578947368425%\n",
      "\n",
      "batch_num: 33.1578947368421%\n",
      "\n",
      "batch_num: 33.68421052631579%\n",
      "\n",
      "batch_num: 34.21052631578947%\n",
      "\n",
      "batch_num: 34.73684210526316%\n",
      "\n",
      "batch_num: 35.26315789473684%\n",
      "\n",
      "batch_num: 35.78947368421053%\n",
      "\n",
      "batch_num: 36.31578947368421%\n",
      "\n",
      "batch_num: 36.84210526315789%\n",
      "\n",
      "batch_num: 37.368421052631575%\n",
      "\n",
      "batch_num: 37.89473684210527%\n",
      "\n",
      "batch_num: 38.421052631578945%\n",
      "\n",
      "batch_num: 38.94736842105263%\n",
      "\n",
      "batch_num: 39.473684210526315%\n",
      "\n",
      "batch_num: 40.0%\n",
      "\n",
      "batch_num: 40.526315789473685%\n",
      "\n",
      "batch_num: 41.05263157894737%\n",
      "\n",
      "batch_num: 41.578947368421055%\n",
      "\n",
      "batch_num: 42.10526315789473%\n",
      "\n",
      "batch_num: 42.63157894736842%\n",
      "\n",
      "batch_num: 43.15789473684211%\n",
      "\n",
      "batch_num: 43.684210526315795%\n",
      "\n",
      "batch_num: 44.21052631578947%\n",
      "\n",
      "batch_num: 44.73684210526316%\n",
      "\n",
      "batch_num: 45.26315789473684%\n",
      "\n",
      "batch_num: 45.78947368421053%\n",
      "\n",
      "batch_num: 46.31578947368421%\n",
      "\n",
      "batch_num: 46.8421052631579%\n",
      "\n",
      "batch_num: 47.368421052631575%\n",
      "\n",
      "batch_num: 47.89473684210526%\n",
      "\n",
      "batch_num: 48.421052631578945%\n",
      "\n",
      "batch_num: 48.94736842105264%\n",
      "\n",
      "batch_num: 49.473684210526315%\n",
      "\n",
      "batch_num: 50.0%\n",
      "\n",
      "batch_num: 50.526315789473685%\n",
      "\n",
      "batch_num: 51.05263157894737%\n",
      "\n",
      "batch_num: 51.578947368421055%\n",
      "\n",
      "batch_num: 52.10526315789473%\n",
      "\n",
      "batch_num: 52.63157894736842%\n",
      "\n",
      "batch_num: 53.1578947368421%\n",
      "\n",
      "batch_num: 53.68421052631579%\n",
      "\n",
      "batch_num: 54.21052631578947%\n",
      "\n",
      "batch_num: 54.736842105263165%\n",
      "\n",
      "batch_num: 55.26315789473685%\n",
      "\n",
      "batch_num: 55.78947368421052%\n",
      "\n",
      "batch_num: 56.315789473684205%\n",
      "\n",
      "batch_num: 56.84210526315789%\n",
      "\n",
      "batch_num: 57.36842105263158%\n",
      "\n",
      "batch_num: 57.89473684210527%\n",
      "\n",
      "batch_num: 58.42105263157895%\n",
      "\n",
      "batch_num: 58.94736842105262%\n",
      "\n",
      "batch_num: 59.473684210526315%\n",
      "\n",
      "batch_num: 60.0%\n",
      "\n",
      "batch_num: 60.526315789473685%\n",
      "\n",
      "batch_num: 61.05263157894737%\n",
      "\n",
      "batch_num: 61.578947368421055%\n",
      "\n",
      "batch_num: 62.10526315789474%\n",
      "\n",
      "batch_num: 62.63157894736842%\n",
      "\n",
      "batch_num: 63.1578947368421%\n",
      "\n",
      "batch_num: 63.68421052631579%\n",
      "\n",
      "batch_num: 64.21052631578948%\n",
      "\n",
      "batch_num: 64.73684210526316%\n",
      "\n",
      "batch_num: 65.26315789473685%\n",
      "\n",
      "batch_num: 65.78947368421053%\n",
      "\n",
      "batch_num: 66.3157894736842%\n",
      "\n",
      "batch_num: 66.84210526315789%\n",
      "\n",
      "batch_num: 67.36842105263158%\n",
      "\n",
      "batch_num: 67.89473684210526%\n",
      "\n",
      "batch_num: 68.42105263157895%\n",
      "\n",
      "batch_num: 68.94736842105263%\n",
      "\n",
      "batch_num: 69.47368421052632%\n",
      "\n",
      "batch_num: 70.0%\n",
      "\n",
      "batch_num: 70.52631578947368%\n",
      "\n",
      "batch_num: 71.05263157894737%\n",
      "\n",
      "batch_num: 71.57894736842105%\n",
      "\n",
      "batch_num: 72.10526315789474%\n",
      "\n",
      "batch_num: 72.63157894736842%\n",
      "\n",
      "batch_num: 73.15789473684211%\n",
      "\n",
      "batch_num: 73.68421052631578%\n",
      "\n",
      "batch_num: 74.21052631578947%\n",
      "\n",
      "batch_num: 74.73684210526315%\n",
      "\n",
      "batch_num: 75.26315789473685%\n",
      "\n",
      "batch_num: 75.78947368421053%\n",
      "\n",
      "batch_num: 76.31578947368422%\n",
      "\n",
      "batch_num: 76.84210526315789%\n",
      "\n",
      "batch_num: 77.36842105263158%\n",
      "\n",
      "batch_num: 77.89473684210526%\n",
      "\n",
      "batch_num: 78.42105263157895%\n",
      "\n",
      "batch_num: 78.94736842105263%\n",
      "\n",
      "batch_num: 79.47368421052632%\n",
      "\n",
      "batch_num: 80.0%\n",
      "\n",
      "batch_num: 80.52631578947368%\n",
      "\n",
      "batch_num: 81.05263157894737%\n",
      "\n",
      "batch_num: 81.57894736842105%\n",
      "\n",
      "batch_num: 82.10526315789474%\n",
      "\n",
      "batch_num: 82.63157894736842%\n",
      "\n",
      "batch_num: 83.15789473684211%\n",
      "\n",
      "batch_num: 83.6842105263158%\n",
      "\n",
      "batch_num: 84.21052631578947%\n",
      "\n",
      "batch_num: 84.73684210526315%\n",
      "\n",
      "batch_num: 85.26315789473684%\n",
      "\n",
      "batch_num: 85.78947368421052%\n",
      "\n",
      "batch_num: 86.31578947368422%\n",
      "\n",
      "batch_num: 86.8421052631579%\n",
      "\n",
      "batch_num: 87.36842105263159%\n",
      "\n",
      "batch_num: 87.89473684210526%\n",
      "\n",
      "batch_num: 88.42105263157895%\n",
      "\n",
      "batch_num: 88.94736842105263%\n",
      "\n",
      "batch_num: 89.47368421052632%\n",
      "\n",
      "batch_num: 90.0%\n",
      "\n",
      "batch_num: 90.52631578947368%\n",
      "\n",
      "batch_num: 91.05263157894737%\n",
      "\n",
      "batch_num: 91.57894736842105%\n",
      "\n",
      "batch_num: 92.10526315789474%\n",
      "\n",
      "batch_num: 92.63157894736842%\n",
      "\n",
      "batch_num: 93.15789473684211%\n",
      "\n",
      "batch_num: 93.6842105263158%\n",
      "\n",
      "batch_num: 94.21052631578948%\n",
      "\n",
      "batch_num: 94.73684210526315%\n",
      "\n",
      "batch_num: 95.26315789473684%\n",
      "\n",
      "batch_num: 95.78947368421052%\n",
      "\n",
      "batch_num: 96.3157894736842%\n",
      "\n",
      "batch_num: 96.84210526315789%\n",
      "\n",
      "batch_num: 97.36842105263158%\n",
      "\n",
      "batch_num: 97.89473684210527%\n",
      "\n",
      "batch_num: 98.42105263157895%\n",
      "\n",
      "batch_num: 98.94736842105263%\n",
      "\n",
      "batch_num: 99.47368421052632%\n",
      "\n",
      "Epoch 1 | train loss: 0.04447866295170235 | val loss: 0.06985765574923293 | accuracy: 97.4%\n",
      "epoch: 2\n",
      "\n",
      "trained: 2\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-15-c5aa7d387409>:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(image, dtype=torch.float), torch.tensor(label_gender, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_num: 0.0%\n",
      "\n",
      "batch_num: 0.5263157894736842%\n",
      "\n",
      "batch_num: 1.0526315789473684%\n",
      "\n",
      "batch_num: 1.5789473684210527%\n",
      "\n",
      "batch_num: 2.1052631578947367%\n",
      "\n",
      "batch_num: 2.631578947368421%\n",
      "\n",
      "batch_num: 3.1578947368421053%\n",
      "\n",
      "batch_num: 3.684210526315789%\n",
      "\n",
      "batch_num: 4.2105263157894735%\n",
      "\n",
      "batch_num: 4.736842105263158%\n",
      "\n",
      "batch_num: 5.263157894736842%\n",
      "\n",
      "batch_num: 5.7894736842105265%\n",
      "\n",
      "batch_num: 6.315789473684211%\n",
      "\n",
      "batch_num: 6.842105263157896%\n",
      "\n",
      "batch_num: 7.368421052631578%\n",
      "\n",
      "batch_num: 7.894736842105263%\n",
      "\n",
      "batch_num: 8.421052631578947%\n",
      "\n",
      "batch_num: 8.947368421052632%\n",
      "\n",
      "batch_num: 9.473684210526317%\n",
      "\n",
      "batch_num: 10.0%\n",
      "\n",
      "batch_num: 10.526315789473683%\n",
      "\n",
      "batch_num: 11.052631578947368%\n",
      "\n",
      "batch_num: 11.578947368421053%\n",
      "\n",
      "batch_num: 12.105263157894736%\n",
      "\n",
      "batch_num: 12.631578947368421%\n",
      "\n",
      "batch_num: 13.157894736842104%\n",
      "\n",
      "batch_num: 13.684210526315791%\n",
      "\n",
      "batch_num: 14.210526315789473%\n",
      "\n",
      "batch_num: 14.736842105263156%\n",
      "\n",
      "batch_num: 15.263157894736842%\n",
      "\n",
      "batch_num: 15.789473684210526%\n",
      "\n",
      "batch_num: 16.315789473684212%\n",
      "\n",
      "batch_num: 16.842105263157894%\n",
      "\n",
      "batch_num: 17.36842105263158%\n",
      "\n",
      "batch_num: 17.894736842105264%\n",
      "\n",
      "batch_num: 18.421052631578945%\n",
      "\n",
      "batch_num: 18.947368421052634%\n",
      "\n",
      "batch_num: 19.473684210526315%\n",
      "\n",
      "batch_num: 20.0%\n",
      "\n",
      "batch_num: 20.526315789473685%\n",
      "\n",
      "batch_num: 21.052631578947366%\n",
      "\n",
      "batch_num: 21.578947368421055%\n",
      "\n",
      "batch_num: 22.105263157894736%\n",
      "\n",
      "batch_num: 22.63157894736842%\n",
      "\n",
      "batch_num: 23.157894736842106%\n",
      "\n",
      "batch_num: 23.684210526315788%\n",
      "\n",
      "batch_num: 24.210526315789473%\n",
      "\n",
      "batch_num: 24.736842105263158%\n",
      "\n",
      "batch_num: 25.263157894736842%\n",
      "\n",
      "batch_num: 25.789473684210527%\n",
      "\n",
      "batch_num: 26.31578947368421%\n",
      "\n",
      "batch_num: 26.842105263157894%\n",
      "\n",
      "batch_num: 27.368421052631582%\n",
      "\n",
      "batch_num: 27.89473684210526%\n",
      "\n",
      "batch_num: 28.421052631578945%\n",
      "\n",
      "batch_num: 28.947368421052634%\n",
      "\n",
      "batch_num: 29.47368421052631%\n",
      "\n",
      "batch_num: 30.0%\n",
      "\n",
      "batch_num: 30.526315789473685%\n",
      "\n",
      "batch_num: 31.05263157894737%\n",
      "\n",
      "batch_num: 31.57894736842105%\n",
      "\n",
      "batch_num: 32.10526315789474%\n",
      "\n",
      "batch_num: 32.631578947368425%\n",
      "\n",
      "batch_num: 33.1578947368421%\n",
      "\n",
      "batch_num: 33.68421052631579%\n",
      "\n",
      "batch_num: 34.21052631578947%\n",
      "\n",
      "batch_num: 34.73684210526316%\n",
      "\n",
      "batch_num: 35.26315789473684%\n",
      "\n",
      "batch_num: 35.78947368421053%\n",
      "\n",
      "batch_num: 36.31578947368421%\n",
      "\n",
      "batch_num: 36.84210526315789%\n",
      "\n",
      "batch_num: 37.368421052631575%\n",
      "\n",
      "batch_num: 37.89473684210527%\n",
      "\n",
      "batch_num: 38.421052631578945%\n",
      "\n",
      "batch_num: 38.94736842105263%\n",
      "\n",
      "batch_num: 39.473684210526315%\n",
      "\n",
      "batch_num: 40.0%\n",
      "\n",
      "batch_num: 40.526315789473685%\n",
      "\n",
      "batch_num: 41.05263157894737%\n",
      "\n",
      "batch_num: 41.578947368421055%\n",
      "\n",
      "batch_num: 42.10526315789473%\n",
      "\n",
      "batch_num: 42.63157894736842%\n",
      "\n",
      "batch_num: 43.15789473684211%\n",
      "\n",
      "batch_num: 43.684210526315795%\n",
      "\n",
      "batch_num: 44.21052631578947%\n",
      "\n",
      "batch_num: 44.73684210526316%\n",
      "\n",
      "batch_num: 45.26315789473684%\n",
      "\n",
      "batch_num: 45.78947368421053%\n",
      "\n",
      "batch_num: 46.31578947368421%\n",
      "\n",
      "batch_num: 46.8421052631579%\n",
      "\n",
      "batch_num: 47.368421052631575%\n",
      "\n",
      "batch_num: 47.89473684210526%\n",
      "\n",
      "batch_num: 48.421052631578945%\n",
      "\n",
      "batch_num: 48.94736842105264%\n",
      "\n",
      "batch_num: 49.473684210526315%\n",
      "\n",
      "batch_num: 50.0%\n",
      "\n",
      "batch_num: 50.526315789473685%\n",
      "\n",
      "batch_num: 51.05263157894737%\n",
      "\n",
      "batch_num: 51.578947368421055%\n",
      "\n",
      "batch_num: 52.10526315789473%\n",
      "\n",
      "batch_num: 52.63157894736842%\n",
      "\n",
      "batch_num: 53.1578947368421%\n",
      "\n",
      "batch_num: 53.68421052631579%\n",
      "\n",
      "batch_num: 54.21052631578947%\n",
      "\n",
      "batch_num: 54.736842105263165%\n",
      "\n",
      "batch_num: 55.26315789473685%\n",
      "\n",
      "batch_num: 55.78947368421052%\n",
      "\n",
      "batch_num: 56.315789473684205%\n",
      "\n",
      "batch_num: 56.84210526315789%\n",
      "\n",
      "batch_num: 57.36842105263158%\n",
      "\n",
      "batch_num: 57.89473684210527%\n",
      "\n",
      "batch_num: 58.42105263157895%\n",
      "\n",
      "batch_num: 58.94736842105262%\n",
      "\n",
      "batch_num: 59.473684210526315%\n",
      "\n",
      "batch_num: 60.0%\n",
      "\n",
      "batch_num: 60.526315789473685%\n",
      "\n",
      "batch_num: 61.05263157894737%\n",
      "\n",
      "batch_num: 61.578947368421055%\n",
      "\n",
      "batch_num: 62.10526315789474%\n",
      "\n",
      "batch_num: 62.63157894736842%\n",
      "\n",
      "batch_num: 63.1578947368421%\n",
      "\n",
      "batch_num: 63.68421052631579%\n",
      "\n",
      "batch_num: 64.21052631578948%\n",
      "\n",
      "batch_num: 64.73684210526316%\n",
      "\n",
      "batch_num: 65.26315789473685%\n",
      "\n",
      "batch_num: 65.78947368421053%\n",
      "\n",
      "batch_num: 66.3157894736842%\n",
      "\n",
      "batch_num: 66.84210526315789%\n",
      "\n",
      "batch_num: 67.36842105263158%\n",
      "\n",
      "batch_num: 67.89473684210526%\n",
      "\n",
      "batch_num: 68.42105263157895%\n",
      "\n",
      "batch_num: 68.94736842105263%\n",
      "\n",
      "batch_num: 69.47368421052632%\n",
      "\n",
      "batch_num: 70.0%\n",
      "\n",
      "batch_num: 70.52631578947368%\n",
      "\n",
      "batch_num: 71.05263157894737%\n",
      "\n",
      "batch_num: 71.57894736842105%\n",
      "\n",
      "batch_num: 72.10526315789474%\n",
      "\n",
      "batch_num: 72.63157894736842%\n",
      "\n",
      "batch_num: 73.15789473684211%\n",
      "\n",
      "batch_num: 73.68421052631578%\n",
      "\n",
      "batch_num: 74.21052631578947%\n",
      "\n",
      "batch_num: 74.73684210526315%\n",
      "\n",
      "batch_num: 75.26315789473685%\n",
      "\n",
      "batch_num: 75.78947368421053%\n",
      "\n",
      "batch_num: 76.31578947368422%\n",
      "\n",
      "batch_num: 76.84210526315789%\n",
      "\n",
      "batch_num: 77.36842105263158%\n",
      "\n",
      "batch_num: 77.89473684210526%\n",
      "\n",
      "batch_num: 78.42105263157895%\n",
      "\n",
      "batch_num: 78.94736842105263%\n",
      "\n",
      "batch_num: 79.47368421052632%\n",
      "\n",
      "batch_num: 80.0%\n",
      "\n",
      "batch_num: 80.52631578947368%\n",
      "\n",
      "batch_num: 81.05263157894737%\n",
      "\n",
      "batch_num: 81.57894736842105%\n",
      "\n",
      "batch_num: 82.10526315789474%\n",
      "\n",
      "batch_num: 82.63157894736842%\n",
      "\n",
      "batch_num: 83.15789473684211%\n",
      "\n",
      "batch_num: 83.6842105263158%\n",
      "\n",
      "batch_num: 84.21052631578947%\n",
      "\n",
      "batch_num: 84.73684210526315%\n",
      "\n",
      "batch_num: 85.26315789473684%\n",
      "\n",
      "batch_num: 85.78947368421052%\n",
      "\n",
      "batch_num: 86.31578947368422%\n",
      "\n",
      "batch_num: 86.8421052631579%\n",
      "\n",
      "batch_num: 87.36842105263159%\n",
      "\n",
      "batch_num: 87.89473684210526%\n",
      "\n",
      "batch_num: 88.42105263157895%\n",
      "\n",
      "batch_num: 88.94736842105263%\n",
      "\n",
      "batch_num: 89.47368421052632%\n",
      "\n",
      "batch_num: 90.0%\n",
      "\n",
      "batch_num: 90.52631578947368%\n",
      "\n",
      "batch_num: 91.05263157894737%\n",
      "\n",
      "batch_num: 91.57894736842105%\n",
      "\n",
      "batch_num: 92.10526315789474%\n",
      "\n",
      "batch_num: 92.63157894736842%\n",
      "\n",
      "batch_num: 93.15789473684211%\n",
      "\n",
      "batch_num: 93.6842105263158%\n",
      "\n",
      "batch_num: 94.21052631578948%\n",
      "\n",
      "batch_num: 94.73684210526315%\n",
      "\n",
      "batch_num: 95.26315789473684%\n",
      "\n",
      "batch_num: 95.78947368421052%\n",
      "\n",
      "batch_num: 96.3157894736842%\n",
      "\n",
      "batch_num: 96.84210526315789%\n",
      "\n",
      "batch_num: 97.36842105263158%\n",
      "\n",
      "batch_num: 97.89473684210527%\n",
      "\n",
      "batch_num: 98.42105263157895%\n",
      "\n",
      "batch_num: 98.94736842105263%\n",
      "\n",
      "batch_num: 99.47368421052632%\n",
      "\n",
      "Epoch 2 | train loss: 0.017705363490447205 | val loss: 0.06993171491194516 | accuracy: 97.92%\n",
      "epoch: 3\n",
      "\n",
      "trained: 3\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-15-c5aa7d387409>:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(image, dtype=torch.float), torch.tensor(label_gender, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_num: 0.0%\n",
      "\n",
      "batch_num: 0.5263157894736842%\n",
      "\n",
      "batch_num: 1.0526315789473684%\n",
      "\n",
      "batch_num: 1.5789473684210527%\n",
      "\n",
      "batch_num: 2.1052631578947367%\n",
      "\n",
      "batch_num: 2.631578947368421%\n",
      "\n",
      "batch_num: 3.1578947368421053%\n",
      "\n",
      "batch_num: 3.684210526315789%\n",
      "\n",
      "batch_num: 4.2105263157894735%\n",
      "\n",
      "batch_num: 4.736842105263158%\n",
      "\n",
      "batch_num: 5.263157894736842%\n",
      "\n",
      "batch_num: 5.7894736842105265%\n",
      "\n",
      "batch_num: 6.315789473684211%\n",
      "\n",
      "batch_num: 6.842105263157896%\n",
      "\n",
      "batch_num: 7.368421052631578%\n",
      "\n",
      "batch_num: 7.894736842105263%\n",
      "\n",
      "batch_num: 8.421052631578947%\n",
      "\n",
      "batch_num: 8.947368421052632%\n",
      "\n",
      "batch_num: 9.473684210526317%\n",
      "\n",
      "batch_num: 10.0%\n",
      "\n",
      "batch_num: 10.526315789473683%\n",
      "\n",
      "batch_num: 11.052631578947368%\n",
      "\n",
      "batch_num: 11.578947368421053%\n",
      "\n",
      "batch_num: 12.105263157894736%\n",
      "\n",
      "batch_num: 12.631578947368421%\n",
      "\n",
      "batch_num: 13.157894736842104%\n",
      "\n",
      "batch_num: 13.684210526315791%\n",
      "\n",
      "batch_num: 14.210526315789473%\n",
      "\n",
      "batch_num: 14.736842105263156%\n",
      "\n",
      "batch_num: 15.263157894736842%\n",
      "\n",
      "batch_num: 15.789473684210526%\n",
      "\n",
      "batch_num: 16.315789473684212%\n",
      "\n",
      "batch_num: 16.842105263157894%\n",
      "\n",
      "batch_num: 17.36842105263158%\n",
      "\n",
      "batch_num: 17.894736842105264%\n",
      "\n",
      "batch_num: 18.421052631578945%\n",
      "\n",
      "batch_num: 18.947368421052634%\n",
      "\n",
      "batch_num: 19.473684210526315%\n",
      "\n",
      "batch_num: 20.0%\n",
      "\n",
      "batch_num: 20.526315789473685%\n",
      "\n",
      "batch_num: 21.052631578947366%\n",
      "\n",
      "batch_num: 21.578947368421055%\n",
      "\n",
      "batch_num: 22.105263157894736%\n",
      "\n",
      "batch_num: 22.63157894736842%\n",
      "\n",
      "batch_num: 23.157894736842106%\n",
      "\n",
      "batch_num: 23.684210526315788%\n",
      "\n",
      "batch_num: 24.210526315789473%\n",
      "\n",
      "batch_num: 24.736842105263158%\n",
      "\n",
      "batch_num: 25.263157894736842%\n",
      "\n",
      "batch_num: 25.789473684210527%\n",
      "\n",
      "batch_num: 26.31578947368421%\n",
      "\n",
      "batch_num: 26.842105263157894%\n",
      "\n",
      "batch_num: 27.368421052631582%\n",
      "\n",
      "batch_num: 27.89473684210526%\n",
      "\n",
      "batch_num: 28.421052631578945%\n",
      "\n",
      "batch_num: 28.947368421052634%\n",
      "\n",
      "batch_num: 29.47368421052631%\n",
      "\n",
      "batch_num: 30.0%\n",
      "\n",
      "batch_num: 30.526315789473685%\n",
      "\n",
      "batch_num: 31.05263157894737%\n",
      "\n",
      "batch_num: 31.57894736842105%\n",
      "\n",
      "batch_num: 32.10526315789474%\n",
      "\n",
      "batch_num: 32.631578947368425%\n",
      "\n",
      "batch_num: 33.1578947368421%\n",
      "\n",
      "batch_num: 33.68421052631579%\n",
      "\n",
      "batch_num: 34.21052631578947%\n",
      "\n",
      "batch_num: 34.73684210526316%\n",
      "\n",
      "batch_num: 35.26315789473684%\n",
      "\n",
      "batch_num: 35.78947368421053%\n",
      "\n",
      "batch_num: 36.31578947368421%\n",
      "\n",
      "batch_num: 36.84210526315789%\n",
      "\n",
      "batch_num: 37.368421052631575%\n",
      "\n",
      "batch_num: 37.89473684210527%\n",
      "\n",
      "batch_num: 38.421052631578945%\n",
      "\n",
      "batch_num: 38.94736842105263%\n",
      "\n",
      "batch_num: 39.473684210526315%\n",
      "\n",
      "batch_num: 40.0%\n",
      "\n",
      "batch_num: 40.526315789473685%\n",
      "\n",
      "batch_num: 41.05263157894737%\n",
      "\n",
      "batch_num: 41.578947368421055%\n",
      "\n",
      "batch_num: 42.10526315789473%\n",
      "\n",
      "batch_num: 42.63157894736842%\n",
      "\n",
      "batch_num: 43.15789473684211%\n",
      "\n",
      "batch_num: 43.684210526315795%\n",
      "\n",
      "batch_num: 44.21052631578947%\n",
      "\n",
      "batch_num: 44.73684210526316%\n",
      "\n",
      "batch_num: 45.26315789473684%\n",
      "\n",
      "batch_num: 45.78947368421053%\n",
      "\n",
      "batch_num: 46.31578947368421%\n",
      "\n",
      "batch_num: 46.8421052631579%\n",
      "\n",
      "batch_num: 47.368421052631575%\n",
      "\n",
      "batch_num: 47.89473684210526%\n",
      "\n",
      "batch_num: 48.421052631578945%\n",
      "\n",
      "batch_num: 48.94736842105264%\n",
      "\n",
      "batch_num: 49.473684210526315%\n",
      "\n",
      "batch_num: 50.0%\n",
      "\n",
      "batch_num: 50.526315789473685%\n",
      "\n",
      "batch_num: 51.05263157894737%\n",
      "\n",
      "batch_num: 51.578947368421055%\n",
      "\n",
      "batch_num: 52.10526315789473%\n",
      "\n",
      "batch_num: 52.63157894736842%\n",
      "\n",
      "batch_num: 53.1578947368421%\n",
      "\n",
      "batch_num: 53.68421052631579%\n",
      "\n",
      "batch_num: 54.21052631578947%\n",
      "\n",
      "batch_num: 54.736842105263165%\n",
      "\n",
      "batch_num: 55.26315789473685%\n",
      "\n",
      "batch_num: 55.78947368421052%\n",
      "\n",
      "batch_num: 56.315789473684205%\n",
      "\n",
      "batch_num: 56.84210526315789%\n",
      "\n",
      "batch_num: 57.36842105263158%\n",
      "\n",
      "batch_num: 57.89473684210527%\n",
      "\n",
      "batch_num: 58.42105263157895%\n",
      "\n",
      "batch_num: 58.94736842105262%\n",
      "\n",
      "batch_num: 59.473684210526315%\n",
      "\n",
      "batch_num: 60.0%\n",
      "\n",
      "batch_num: 60.526315789473685%\n",
      "\n",
      "batch_num: 61.05263157894737%\n",
      "\n",
      "batch_num: 61.578947368421055%\n",
      "\n",
      "batch_num: 62.10526315789474%\n",
      "\n",
      "batch_num: 62.63157894736842%\n",
      "\n",
      "batch_num: 63.1578947368421%\n",
      "\n",
      "batch_num: 63.68421052631579%\n",
      "\n",
      "batch_num: 64.21052631578948%\n",
      "\n",
      "batch_num: 64.73684210526316%\n",
      "\n",
      "batch_num: 65.26315789473685%\n",
      "\n",
      "batch_num: 65.78947368421053%\n",
      "\n",
      "batch_num: 66.3157894736842%\n",
      "\n",
      "batch_num: 66.84210526315789%\n",
      "\n",
      "batch_num: 67.36842105263158%\n",
      "\n",
      "batch_num: 67.89473684210526%\n",
      "\n",
      "batch_num: 68.42105263157895%\n",
      "\n",
      "batch_num: 68.94736842105263%\n",
      "\n",
      "batch_num: 69.47368421052632%\n",
      "\n",
      "batch_num: 70.0%\n",
      "\n",
      "batch_num: 70.52631578947368%\n",
      "\n",
      "batch_num: 71.05263157894737%\n",
      "\n",
      "batch_num: 71.57894736842105%\n",
      "\n",
      "batch_num: 72.10526315789474%\n",
      "\n",
      "batch_num: 72.63157894736842%\n",
      "\n",
      "batch_num: 73.15789473684211%\n",
      "\n",
      "batch_num: 73.68421052631578%\n",
      "\n",
      "batch_num: 74.21052631578947%\n",
      "\n",
      "batch_num: 74.73684210526315%\n",
      "\n",
      "batch_num: 75.26315789473685%\n",
      "\n",
      "batch_num: 75.78947368421053%\n",
      "\n",
      "batch_num: 76.31578947368422%\n",
      "\n",
      "batch_num: 76.84210526315789%\n",
      "\n",
      "batch_num: 77.36842105263158%\n",
      "\n",
      "batch_num: 77.89473684210526%\n",
      "\n",
      "batch_num: 78.42105263157895%\n",
      "\n",
      "batch_num: 78.94736842105263%\n",
      "\n",
      "batch_num: 79.47368421052632%\n",
      "\n",
      "batch_num: 80.0%\n",
      "\n",
      "batch_num: 80.52631578947368%\n",
      "\n",
      "batch_num: 81.05263157894737%\n",
      "\n",
      "batch_num: 81.57894736842105%\n",
      "\n",
      "batch_num: 82.10526315789474%\n",
      "\n",
      "batch_num: 82.63157894736842%\n",
      "\n",
      "batch_num: 83.15789473684211%\n",
      "\n",
      "batch_num: 83.6842105263158%\n",
      "\n",
      "batch_num: 84.21052631578947%\n",
      "\n",
      "batch_num: 84.73684210526315%\n",
      "\n",
      "batch_num: 85.26315789473684%\n",
      "\n",
      "batch_num: 85.78947368421052%\n",
      "\n",
      "batch_num: 86.31578947368422%\n",
      "\n",
      "batch_num: 86.8421052631579%\n",
      "\n",
      "batch_num: 87.36842105263159%\n",
      "\n",
      "batch_num: 87.89473684210526%\n",
      "\n",
      "batch_num: 88.42105263157895%\n",
      "\n",
      "batch_num: 88.94736842105263%\n",
      "\n",
      "batch_num: 89.47368421052632%\n",
      "\n",
      "batch_num: 90.0%\n",
      "\n",
      "batch_num: 90.52631578947368%\n",
      "\n",
      "batch_num: 91.05263157894737%\n",
      "\n",
      "batch_num: 91.57894736842105%\n",
      "\n",
      "batch_num: 92.10526315789474%\n",
      "\n",
      "batch_num: 92.63157894736842%\n",
      "\n",
      "batch_num: 93.15789473684211%\n",
      "\n",
      "batch_num: 93.6842105263158%\n",
      "\n",
      "batch_num: 94.21052631578948%\n",
      "\n",
      "batch_num: 94.73684210526315%\n",
      "\n",
      "batch_num: 95.26315789473684%\n",
      "\n",
      "batch_num: 95.78947368421052%\n",
      "\n",
      "batch_num: 96.3157894736842%\n",
      "\n",
      "batch_num: 96.84210526315789%\n",
      "\n",
      "batch_num: 97.36842105263158%\n",
      "\n",
      "batch_num: 97.89473684210527%\n",
      "\n",
      "batch_num: 98.42105263157895%\n",
      "\n",
      "batch_num: 98.94736842105263%\n",
      "\n",
      "batch_num: 99.47368421052632%\n",
      "\n",
      "Epoch 3 | train loss: 0.012073179669862025 | val loss: 0.07923092396760528 | accuracy: 97.7%\n",
      "epoch: 4\n",
      "\n",
      "trained: 4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-15-c5aa7d387409>:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(image, dtype=torch.float), torch.tensor(label_gender, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_num: 0.0%\n",
      "\n",
      "batch_num: 0.5263157894736842%\n",
      "\n",
      "batch_num: 1.0526315789473684%\n",
      "\n",
      "batch_num: 1.5789473684210527%\n",
      "\n",
      "batch_num: 2.1052631578947367%\n",
      "\n",
      "batch_num: 2.631578947368421%\n",
      "\n",
      "batch_num: 3.1578947368421053%\n",
      "\n",
      "batch_num: 3.684210526315789%\n",
      "\n",
      "batch_num: 4.2105263157894735%\n",
      "\n",
      "batch_num: 4.736842105263158%\n",
      "\n",
      "batch_num: 5.263157894736842%\n",
      "\n",
      "batch_num: 5.7894736842105265%\n",
      "\n",
      "batch_num: 6.315789473684211%\n",
      "\n",
      "batch_num: 6.842105263157896%\n",
      "\n",
      "batch_num: 7.368421052631578%\n",
      "\n",
      "batch_num: 7.894736842105263%\n",
      "\n",
      "batch_num: 8.421052631578947%\n",
      "\n",
      "batch_num: 8.947368421052632%\n",
      "\n",
      "batch_num: 9.473684210526317%\n",
      "\n",
      "batch_num: 10.0%\n",
      "\n",
      "batch_num: 10.526315789473683%\n",
      "\n",
      "batch_num: 11.052631578947368%\n",
      "\n",
      "batch_num: 11.578947368421053%\n",
      "\n",
      "batch_num: 12.105263157894736%\n",
      "\n",
      "batch_num: 12.631578947368421%\n",
      "\n",
      "batch_num: 13.157894736842104%\n",
      "\n",
      "batch_num: 13.684210526315791%\n",
      "\n",
      "batch_num: 14.210526315789473%\n",
      "\n",
      "batch_num: 14.736842105263156%\n",
      "\n",
      "batch_num: 15.263157894736842%\n",
      "\n",
      "batch_num: 15.789473684210526%\n",
      "\n",
      "batch_num: 16.315789473684212%\n",
      "\n",
      "batch_num: 16.842105263157894%\n",
      "\n",
      "batch_num: 17.36842105263158%\n",
      "\n",
      "batch_num: 17.894736842105264%\n",
      "\n",
      "batch_num: 18.421052631578945%\n",
      "\n",
      "batch_num: 18.947368421052634%\n",
      "\n",
      "batch_num: 19.473684210526315%\n",
      "\n",
      "batch_num: 20.0%\n",
      "\n",
      "batch_num: 20.526315789473685%\n",
      "\n",
      "batch_num: 21.052631578947366%\n",
      "\n",
      "batch_num: 21.578947368421055%\n",
      "\n",
      "batch_num: 22.105263157894736%\n",
      "\n",
      "batch_num: 22.63157894736842%\n",
      "\n",
      "batch_num: 23.157894736842106%\n",
      "\n",
      "batch_num: 23.684210526315788%\n",
      "\n",
      "batch_num: 24.210526315789473%\n",
      "\n",
      "batch_num: 24.736842105263158%\n",
      "\n",
      "batch_num: 25.263157894736842%\n",
      "\n",
      "batch_num: 25.789473684210527%\n",
      "\n",
      "batch_num: 26.31578947368421%\n",
      "\n",
      "batch_num: 26.842105263157894%\n",
      "\n",
      "batch_num: 27.368421052631582%\n",
      "\n",
      "batch_num: 27.89473684210526%\n",
      "\n",
      "batch_num: 28.421052631578945%\n",
      "\n",
      "batch_num: 28.947368421052634%\n",
      "\n",
      "batch_num: 29.47368421052631%\n",
      "\n",
      "batch_num: 30.0%\n",
      "\n",
      "batch_num: 30.526315789473685%\n",
      "\n",
      "batch_num: 31.05263157894737%\n",
      "\n",
      "batch_num: 31.57894736842105%\n",
      "\n",
      "batch_num: 32.10526315789474%\n",
      "\n",
      "batch_num: 32.631578947368425%\n",
      "\n",
      "batch_num: 33.1578947368421%\n",
      "\n",
      "batch_num: 33.68421052631579%\n",
      "\n",
      "batch_num: 34.21052631578947%\n",
      "\n",
      "batch_num: 34.73684210526316%\n",
      "\n",
      "batch_num: 35.26315789473684%\n",
      "\n",
      "batch_num: 35.78947368421053%\n",
      "\n",
      "batch_num: 36.31578947368421%\n",
      "\n",
      "batch_num: 36.84210526315789%\n",
      "\n",
      "batch_num: 37.368421052631575%\n",
      "\n",
      "batch_num: 37.89473684210527%\n",
      "\n",
      "batch_num: 38.421052631578945%\n",
      "\n",
      "batch_num: 38.94736842105263%\n",
      "\n",
      "batch_num: 39.473684210526315%\n",
      "\n",
      "batch_num: 40.0%\n",
      "\n",
      "batch_num: 40.526315789473685%\n",
      "\n",
      "batch_num: 41.05263157894737%\n",
      "\n",
      "batch_num: 41.578947368421055%\n",
      "\n",
      "batch_num: 42.10526315789473%\n",
      "\n",
      "batch_num: 42.63157894736842%\n",
      "\n",
      "batch_num: 43.15789473684211%\n",
      "\n",
      "batch_num: 43.684210526315795%\n",
      "\n",
      "batch_num: 44.21052631578947%\n",
      "\n",
      "batch_num: 44.73684210526316%\n",
      "\n",
      "batch_num: 45.26315789473684%\n",
      "\n",
      "batch_num: 45.78947368421053%\n",
      "\n",
      "batch_num: 46.31578947368421%\n",
      "\n",
      "batch_num: 46.8421052631579%\n",
      "\n",
      "batch_num: 47.368421052631575%\n",
      "\n",
      "batch_num: 47.89473684210526%\n",
      "\n",
      "batch_num: 48.421052631578945%\n",
      "\n",
      "batch_num: 48.94736842105264%\n",
      "\n",
      "batch_num: 49.473684210526315%\n",
      "\n",
      "batch_num: 50.0%\n",
      "\n",
      "batch_num: 50.526315789473685%\n",
      "\n",
      "batch_num: 51.05263157894737%\n",
      "\n",
      "batch_num: 51.578947368421055%\n",
      "\n",
      "batch_num: 52.10526315789473%\n",
      "\n",
      "batch_num: 52.63157894736842%\n",
      "\n",
      "batch_num: 53.1578947368421%\n",
      "\n",
      "batch_num: 53.68421052631579%\n",
      "\n",
      "batch_num: 54.21052631578947%\n",
      "\n",
      "batch_num: 54.736842105263165%\n",
      "\n",
      "batch_num: 55.26315789473685%\n",
      "\n",
      "batch_num: 55.78947368421052%\n",
      "\n",
      "batch_num: 56.315789473684205%\n",
      "\n",
      "batch_num: 56.84210526315789%\n",
      "\n",
      "batch_num: 57.36842105263158%\n",
      "\n",
      "batch_num: 57.89473684210527%\n",
      "\n",
      "batch_num: 58.42105263157895%\n",
      "\n",
      "batch_num: 58.94736842105262%\n",
      "\n",
      "batch_num: 59.473684210526315%\n",
      "\n",
      "batch_num: 60.0%\n",
      "\n",
      "batch_num: 60.526315789473685%\n",
      "\n",
      "batch_num: 61.05263157894737%\n",
      "\n",
      "batch_num: 61.578947368421055%\n",
      "\n",
      "batch_num: 62.10526315789474%\n",
      "\n",
      "batch_num: 62.63157894736842%\n",
      "\n",
      "batch_num: 63.1578947368421%\n",
      "\n",
      "batch_num: 63.68421052631579%\n",
      "\n",
      "batch_num: 64.21052631578948%\n",
      "\n",
      "batch_num: 64.73684210526316%\n",
      "\n",
      "batch_num: 65.26315789473685%\n",
      "\n",
      "batch_num: 65.78947368421053%\n",
      "\n",
      "batch_num: 66.3157894736842%\n",
      "\n",
      "batch_num: 66.84210526315789%\n",
      "\n",
      "batch_num: 67.36842105263158%\n",
      "\n",
      "batch_num: 67.89473684210526%\n",
      "\n",
      "batch_num: 68.42105263157895%\n",
      "\n",
      "batch_num: 68.94736842105263%\n",
      "\n",
      "batch_num: 69.47368421052632%\n",
      "\n",
      "batch_num: 70.0%\n",
      "\n",
      "batch_num: 70.52631578947368%\n",
      "\n",
      "batch_num: 71.05263157894737%\n",
      "\n",
      "batch_num: 71.57894736842105%\n",
      "\n",
      "batch_num: 72.10526315789474%\n",
      "\n",
      "batch_num: 72.63157894736842%\n",
      "\n",
      "batch_num: 73.15789473684211%\n",
      "\n",
      "batch_num: 73.68421052631578%\n",
      "\n",
      "batch_num: 74.21052631578947%\n",
      "\n",
      "batch_num: 74.73684210526315%\n",
      "\n",
      "batch_num: 75.26315789473685%\n",
      "\n",
      "batch_num: 75.78947368421053%\n",
      "\n",
      "batch_num: 76.31578947368422%\n",
      "\n",
      "batch_num: 76.84210526315789%\n",
      "\n",
      "batch_num: 77.36842105263158%\n",
      "\n",
      "batch_num: 77.89473684210526%\n",
      "\n",
      "batch_num: 78.42105263157895%\n",
      "\n",
      "batch_num: 78.94736842105263%\n",
      "\n",
      "batch_num: 79.47368421052632%\n",
      "\n",
      "batch_num: 80.0%\n",
      "\n",
      "batch_num: 80.52631578947368%\n",
      "\n",
      "batch_num: 81.05263157894737%\n",
      "\n",
      "batch_num: 81.57894736842105%\n",
      "\n",
      "batch_num: 82.10526315789474%\n",
      "\n",
      "batch_num: 82.63157894736842%\n",
      "\n",
      "batch_num: 83.15789473684211%\n",
      "\n",
      "batch_num: 83.6842105263158%\n",
      "\n",
      "batch_num: 84.21052631578947%\n",
      "\n",
      "batch_num: 84.73684210526315%\n",
      "\n",
      "batch_num: 85.26315789473684%\n",
      "\n",
      "batch_num: 85.78947368421052%\n",
      "\n",
      "batch_num: 86.31578947368422%\n",
      "\n",
      "batch_num: 86.8421052631579%\n",
      "\n",
      "batch_num: 87.36842105263159%\n",
      "\n",
      "batch_num: 87.89473684210526%\n",
      "\n",
      "batch_num: 88.42105263157895%\n",
      "\n",
      "batch_num: 88.94736842105263%\n",
      "\n",
      "batch_num: 89.47368421052632%\n",
      "\n",
      "batch_num: 90.0%\n",
      "\n",
      "batch_num: 90.52631578947368%\n",
      "\n",
      "batch_num: 91.05263157894737%\n",
      "\n",
      "batch_num: 91.57894736842105%\n",
      "\n",
      "batch_num: 92.10526315789474%\n",
      "\n",
      "batch_num: 92.63157894736842%\n",
      "\n",
      "batch_num: 93.15789473684211%\n",
      "\n",
      "batch_num: 93.6842105263158%\n",
      "\n",
      "batch_num: 94.21052631578948%\n",
      "\n",
      "batch_num: 94.73684210526315%\n",
      "\n",
      "batch_num: 95.26315789473684%\n",
      "\n",
      "batch_num: 95.78947368421052%\n",
      "\n",
      "batch_num: 96.3157894736842%\n",
      "\n",
      "batch_num: 96.84210526315789%\n",
      "\n",
      "batch_num: 97.36842105263158%\n",
      "\n",
      "batch_num: 97.89473684210527%\n",
      "\n",
      "batch_num: 98.42105263157895%\n",
      "\n",
      "batch_num: 98.94736842105263%\n",
      "\n",
      "batch_num: 99.47368421052632%\n",
      "\n",
      "Epoch 4 | train loss: 0.008519062112502165 | val loss: 0.07923667870371984 | accuracy: 97.99%\n",
      "epoch: 5\n",
      "\n",
      "trained: 5\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-15-c5aa7d387409>:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(image, dtype=torch.float), torch.tensor(label_gender, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_num: 0.0%\n",
      "\n",
      "batch_num: 0.5263157894736842%\n",
      "\n",
      "batch_num: 1.0526315789473684%\n",
      "\n",
      "batch_num: 1.5789473684210527%\n",
      "\n",
      "batch_num: 2.1052631578947367%\n",
      "\n",
      "batch_num: 2.631578947368421%\n",
      "\n",
      "batch_num: 3.1578947368421053%\n",
      "\n",
      "batch_num: 3.684210526315789%\n",
      "\n",
      "batch_num: 4.2105263157894735%\n",
      "\n",
      "batch_num: 4.736842105263158%\n",
      "\n",
      "batch_num: 5.263157894736842%\n",
      "\n",
      "batch_num: 5.7894736842105265%\n",
      "\n",
      "batch_num: 6.315789473684211%\n",
      "\n",
      "batch_num: 6.842105263157896%\n",
      "\n",
      "batch_num: 7.368421052631578%\n",
      "\n",
      "batch_num: 7.894736842105263%\n",
      "\n",
      "batch_num: 8.421052631578947%\n",
      "\n",
      "batch_num: 8.947368421052632%\n",
      "\n",
      "batch_num: 9.473684210526317%\n",
      "\n",
      "batch_num: 10.0%\n",
      "\n",
      "batch_num: 10.526315789473683%\n",
      "\n",
      "batch_num: 11.052631578947368%\n",
      "\n",
      "batch_num: 11.578947368421053%\n",
      "\n",
      "batch_num: 12.105263157894736%\n",
      "\n",
      "batch_num: 12.631578947368421%\n",
      "\n",
      "batch_num: 13.157894736842104%\n",
      "\n",
      "batch_num: 13.684210526315791%\n",
      "\n",
      "batch_num: 14.210526315789473%\n",
      "\n",
      "batch_num: 14.736842105263156%\n",
      "\n",
      "batch_num: 15.263157894736842%\n",
      "\n",
      "batch_num: 15.789473684210526%\n",
      "\n",
      "batch_num: 16.315789473684212%\n",
      "\n",
      "batch_num: 16.842105263157894%\n",
      "\n",
      "batch_num: 17.36842105263158%\n",
      "\n",
      "batch_num: 17.894736842105264%\n",
      "\n",
      "batch_num: 18.421052631578945%\n",
      "\n",
      "batch_num: 18.947368421052634%\n",
      "\n",
      "batch_num: 19.473684210526315%\n",
      "\n",
      "batch_num: 20.0%\n",
      "\n",
      "batch_num: 20.526315789473685%\n",
      "\n",
      "batch_num: 21.052631578947366%\n",
      "\n",
      "batch_num: 21.578947368421055%\n",
      "\n",
      "batch_num: 22.105263157894736%\n",
      "\n",
      "batch_num: 22.63157894736842%\n",
      "\n",
      "batch_num: 23.157894736842106%\n",
      "\n",
      "batch_num: 23.684210526315788%\n",
      "\n",
      "batch_num: 24.210526315789473%\n",
      "\n",
      "batch_num: 24.736842105263158%\n",
      "\n",
      "batch_num: 25.263157894736842%\n",
      "\n",
      "batch_num: 25.789473684210527%\n",
      "\n",
      "batch_num: 26.31578947368421%\n",
      "\n",
      "batch_num: 26.842105263157894%\n",
      "\n",
      "batch_num: 27.368421052631582%\n",
      "\n",
      "batch_num: 27.89473684210526%\n",
      "\n",
      "batch_num: 28.421052631578945%\n",
      "\n",
      "batch_num: 28.947368421052634%\n",
      "\n",
      "batch_num: 29.47368421052631%\n",
      "\n",
      "batch_num: 30.0%\n",
      "\n",
      "batch_num: 30.526315789473685%\n",
      "\n",
      "batch_num: 31.05263157894737%\n",
      "\n",
      "batch_num: 31.57894736842105%\n",
      "\n",
      "batch_num: 32.10526315789474%\n",
      "\n",
      "batch_num: 32.631578947368425%\n",
      "\n",
      "batch_num: 33.1578947368421%\n",
      "\n",
      "batch_num: 33.68421052631579%\n",
      "\n",
      "batch_num: 34.21052631578947%\n",
      "\n",
      "batch_num: 34.73684210526316%\n",
      "\n",
      "batch_num: 35.26315789473684%\n",
      "\n",
      "batch_num: 35.78947368421053%\n",
      "\n",
      "batch_num: 36.31578947368421%\n",
      "\n",
      "batch_num: 36.84210526315789%\n",
      "\n",
      "batch_num: 37.368421052631575%\n",
      "\n",
      "batch_num: 37.89473684210527%\n",
      "\n",
      "batch_num: 38.421052631578945%\n",
      "\n",
      "batch_num: 38.94736842105263%\n",
      "\n",
      "batch_num: 39.473684210526315%\n",
      "\n",
      "batch_num: 40.0%\n",
      "\n",
      "batch_num: 40.526315789473685%\n",
      "\n",
      "batch_num: 41.05263157894737%\n",
      "\n",
      "batch_num: 41.578947368421055%\n",
      "\n",
      "batch_num: 42.10526315789473%\n",
      "\n",
      "batch_num: 42.63157894736842%\n",
      "\n",
      "batch_num: 43.15789473684211%\n",
      "\n",
      "batch_num: 43.684210526315795%\n",
      "\n",
      "batch_num: 44.21052631578947%\n",
      "\n",
      "batch_num: 44.73684210526316%\n",
      "\n",
      "batch_num: 45.26315789473684%\n",
      "\n",
      "batch_num: 45.78947368421053%\n",
      "\n",
      "batch_num: 46.31578947368421%\n",
      "\n",
      "batch_num: 46.8421052631579%\n",
      "\n",
      "batch_num: 47.368421052631575%\n",
      "\n",
      "batch_num: 47.89473684210526%\n",
      "\n",
      "batch_num: 48.421052631578945%\n",
      "\n",
      "batch_num: 48.94736842105264%\n",
      "\n",
      "batch_num: 49.473684210526315%\n",
      "\n",
      "batch_num: 50.0%\n",
      "\n",
      "batch_num: 50.526315789473685%\n",
      "\n",
      "batch_num: 51.05263157894737%\n",
      "\n",
      "batch_num: 51.578947368421055%\n",
      "\n",
      "batch_num: 52.10526315789473%\n",
      "\n",
      "batch_num: 52.63157894736842%\n",
      "\n",
      "batch_num: 53.1578947368421%\n",
      "\n",
      "batch_num: 53.68421052631579%\n",
      "\n",
      "batch_num: 54.21052631578947%\n",
      "\n",
      "batch_num: 54.736842105263165%\n",
      "\n",
      "batch_num: 55.26315789473685%\n",
      "\n",
      "batch_num: 55.78947368421052%\n",
      "\n",
      "batch_num: 56.315789473684205%\n",
      "\n",
      "batch_num: 56.84210526315789%\n",
      "\n",
      "batch_num: 57.36842105263158%\n",
      "\n",
      "batch_num: 57.89473684210527%\n",
      "\n",
      "batch_num: 58.42105263157895%\n",
      "\n",
      "batch_num: 58.94736842105262%\n",
      "\n",
      "batch_num: 59.473684210526315%\n",
      "\n",
      "batch_num: 60.0%\n",
      "\n",
      "batch_num: 60.526315789473685%\n",
      "\n",
      "batch_num: 61.05263157894737%\n",
      "\n",
      "batch_num: 61.578947368421055%\n",
      "\n",
      "batch_num: 62.10526315789474%\n",
      "\n",
      "batch_num: 62.63157894736842%\n",
      "\n",
      "batch_num: 63.1578947368421%\n",
      "\n",
      "batch_num: 63.68421052631579%\n",
      "\n",
      "batch_num: 64.21052631578948%\n",
      "\n",
      "batch_num: 64.73684210526316%\n",
      "\n",
      "batch_num: 65.26315789473685%\n",
      "\n",
      "batch_num: 65.78947368421053%\n",
      "\n",
      "batch_num: 66.3157894736842%\n",
      "\n",
      "batch_num: 66.84210526315789%\n",
      "\n",
      "batch_num: 67.36842105263158%\n",
      "\n",
      "batch_num: 67.89473684210526%\n",
      "\n",
      "batch_num: 68.42105263157895%\n",
      "\n",
      "batch_num: 68.94736842105263%\n",
      "\n",
      "batch_num: 69.47368421052632%\n",
      "\n",
      "batch_num: 70.0%\n",
      "\n",
      "batch_num: 70.52631578947368%\n",
      "\n",
      "batch_num: 71.05263157894737%\n",
      "\n",
      "batch_num: 71.57894736842105%\n",
      "\n",
      "batch_num: 72.10526315789474%\n",
      "\n",
      "batch_num: 72.63157894736842%\n",
      "\n",
      "batch_num: 73.15789473684211%\n",
      "\n",
      "batch_num: 73.68421052631578%\n",
      "\n",
      "batch_num: 74.21052631578947%\n",
      "\n",
      "batch_num: 74.73684210526315%\n",
      "\n",
      "batch_num: 75.26315789473685%\n",
      "\n",
      "batch_num: 75.78947368421053%\n",
      "\n",
      "batch_num: 76.31578947368422%\n",
      "\n",
      "batch_num: 76.84210526315789%\n",
      "\n",
      "batch_num: 77.36842105263158%\n",
      "\n",
      "batch_num: 77.89473684210526%\n",
      "\n",
      "batch_num: 78.42105263157895%\n",
      "\n",
      "batch_num: 78.94736842105263%\n",
      "\n",
      "batch_num: 79.47368421052632%\n",
      "\n",
      "batch_num: 80.0%\n",
      "\n",
      "batch_num: 80.52631578947368%\n",
      "\n",
      "batch_num: 81.05263157894737%\n",
      "\n",
      "batch_num: 81.57894736842105%\n",
      "\n",
      "batch_num: 82.10526315789474%\n",
      "\n",
      "batch_num: 82.63157894736842%\n",
      "\n",
      "batch_num: 83.15789473684211%\n",
      "\n",
      "batch_num: 83.6842105263158%\n",
      "\n",
      "batch_num: 84.21052631578947%\n",
      "\n",
      "batch_num: 84.73684210526315%\n",
      "\n",
      "batch_num: 85.26315789473684%\n",
      "\n",
      "batch_num: 85.78947368421052%\n",
      "\n",
      "batch_num: 86.31578947368422%\n",
      "\n",
      "batch_num: 86.8421052631579%\n",
      "\n",
      "batch_num: 87.36842105263159%\n",
      "\n",
      "batch_num: 87.89473684210526%\n",
      "\n",
      "batch_num: 88.42105263157895%\n",
      "\n",
      "batch_num: 88.94736842105263%\n",
      "\n",
      "batch_num: 89.47368421052632%\n",
      "\n",
      "batch_num: 90.0%\n",
      "\n",
      "batch_num: 90.52631578947368%\n",
      "\n",
      "batch_num: 91.05263157894737%\n",
      "\n",
      "batch_num: 91.57894736842105%\n",
      "\n",
      "batch_num: 92.10526315789474%\n",
      "\n",
      "batch_num: 92.63157894736842%\n",
      "\n",
      "batch_num: 93.15789473684211%\n",
      "\n",
      "batch_num: 93.6842105263158%\n",
      "\n",
      "batch_num: 94.21052631578948%\n",
      "\n",
      "batch_num: 94.73684210526315%\n",
      "\n",
      "batch_num: 95.26315789473684%\n",
      "\n",
      "batch_num: 95.78947368421052%\n",
      "\n",
      "batch_num: 96.3157894736842%\n",
      "\n",
      "batch_num: 96.84210526315789%\n",
      "\n",
      "batch_num: 97.36842105263158%\n",
      "\n",
      "batch_num: 97.89473684210527%\n",
      "\n",
      "batch_num: 98.42105263157895%\n",
      "\n",
      "batch_num: 98.94736842105263%\n",
      "\n",
      "batch_num: 99.47368421052632%\n",
      "\n",
      "Epoch 5 | train loss: 0.0029845478954436994 | val loss: 0.07597543543670326 | accuracy: 97.85%\n",
      "epoch: 6\n",
      "\n",
      "trained: 6\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-15-c5aa7d387409>:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(image, dtype=torch.float), torch.tensor(label_gender, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_num: 0.0%\n",
      "\n",
      "batch_num: 0.5263157894736842%\n",
      "\n",
      "batch_num: 1.0526315789473684%\n",
      "\n",
      "batch_num: 1.5789473684210527%\n",
      "\n",
      "batch_num: 2.1052631578947367%\n",
      "\n",
      "batch_num: 2.631578947368421%\n",
      "\n",
      "batch_num: 3.1578947368421053%\n",
      "\n",
      "batch_num: 3.684210526315789%\n",
      "\n",
      "batch_num: 4.2105263157894735%\n",
      "\n",
      "batch_num: 4.736842105263158%\n",
      "\n",
      "batch_num: 5.263157894736842%\n",
      "\n",
      "batch_num: 5.7894736842105265%\n",
      "\n",
      "batch_num: 6.315789473684211%\n",
      "\n",
      "batch_num: 6.842105263157896%\n",
      "\n",
      "batch_num: 7.368421052631578%\n",
      "\n",
      "batch_num: 7.894736842105263%\n",
      "\n",
      "batch_num: 8.421052631578947%\n",
      "\n",
      "batch_num: 8.947368421052632%\n",
      "\n",
      "batch_num: 9.473684210526317%\n",
      "\n",
      "batch_num: 10.0%\n",
      "\n",
      "batch_num: 10.526315789473683%\n",
      "\n",
      "batch_num: 11.052631578947368%\n",
      "\n",
      "batch_num: 11.578947368421053%\n",
      "\n",
      "batch_num: 12.105263157894736%\n",
      "\n",
      "batch_num: 12.631578947368421%\n",
      "\n",
      "batch_num: 13.157894736842104%\n",
      "\n",
      "batch_num: 13.684210526315791%\n",
      "\n",
      "batch_num: 14.210526315789473%\n",
      "\n",
      "batch_num: 14.736842105263156%\n",
      "\n",
      "batch_num: 15.263157894736842%\n",
      "\n",
      "batch_num: 15.789473684210526%\n",
      "\n",
      "batch_num: 16.315789473684212%\n",
      "\n",
      "batch_num: 16.842105263157894%\n",
      "\n",
      "batch_num: 17.36842105263158%\n",
      "\n",
      "batch_num: 17.894736842105264%\n",
      "\n",
      "batch_num: 18.421052631578945%\n",
      "\n",
      "batch_num: 18.947368421052634%\n",
      "\n",
      "batch_num: 19.473684210526315%\n",
      "\n",
      "batch_num: 20.0%\n",
      "\n",
      "batch_num: 20.526315789473685%\n",
      "\n",
      "batch_num: 21.052631578947366%\n",
      "\n",
      "batch_num: 21.578947368421055%\n",
      "\n",
      "batch_num: 22.105263157894736%\n",
      "\n",
      "batch_num: 22.63157894736842%\n",
      "\n",
      "batch_num: 23.157894736842106%\n",
      "\n",
      "batch_num: 23.684210526315788%\n",
      "\n",
      "batch_num: 24.210526315789473%\n",
      "\n",
      "batch_num: 24.736842105263158%\n",
      "\n",
      "batch_num: 25.263157894736842%\n",
      "\n",
      "batch_num: 25.789473684210527%\n",
      "\n",
      "batch_num: 26.31578947368421%\n",
      "\n",
      "batch_num: 26.842105263157894%\n",
      "\n",
      "batch_num: 27.368421052631582%\n",
      "\n",
      "batch_num: 27.89473684210526%\n",
      "\n",
      "batch_num: 28.421052631578945%\n",
      "\n",
      "batch_num: 28.947368421052634%\n",
      "\n",
      "batch_num: 29.47368421052631%\n",
      "\n",
      "batch_num: 30.0%\n",
      "\n",
      "batch_num: 30.526315789473685%\n",
      "\n",
      "batch_num: 31.05263157894737%\n",
      "\n",
      "batch_num: 31.57894736842105%\n",
      "\n",
      "batch_num: 32.10526315789474%\n",
      "\n",
      "batch_num: 32.631578947368425%\n",
      "\n",
      "batch_num: 33.1578947368421%\n",
      "\n",
      "batch_num: 33.68421052631579%\n",
      "\n",
      "batch_num: 34.21052631578947%\n",
      "\n",
      "batch_num: 34.73684210526316%\n",
      "\n",
      "batch_num: 35.26315789473684%\n",
      "\n",
      "batch_num: 35.78947368421053%\n",
      "\n",
      "batch_num: 36.31578947368421%\n",
      "\n",
      "batch_num: 36.84210526315789%\n",
      "\n",
      "batch_num: 37.368421052631575%\n",
      "\n",
      "batch_num: 37.89473684210527%\n",
      "\n",
      "batch_num: 38.421052631578945%\n",
      "\n",
      "batch_num: 38.94736842105263%\n",
      "\n",
      "batch_num: 39.473684210526315%\n",
      "\n",
      "batch_num: 40.0%\n",
      "\n",
      "batch_num: 40.526315789473685%\n",
      "\n",
      "batch_num: 41.05263157894737%\n",
      "\n",
      "batch_num: 41.578947368421055%\n",
      "\n",
      "batch_num: 42.10526315789473%\n",
      "\n",
      "batch_num: 42.63157894736842%\n",
      "\n",
      "batch_num: 43.15789473684211%\n",
      "\n",
      "batch_num: 43.684210526315795%\n",
      "\n",
      "batch_num: 44.21052631578947%\n",
      "\n",
      "batch_num: 44.73684210526316%\n",
      "\n",
      "batch_num: 45.26315789473684%\n",
      "\n",
      "batch_num: 45.78947368421053%\n",
      "\n",
      "batch_num: 46.31578947368421%\n",
      "\n",
      "batch_num: 46.8421052631579%\n",
      "\n",
      "batch_num: 47.368421052631575%\n",
      "\n",
      "batch_num: 47.89473684210526%\n",
      "\n",
      "batch_num: 48.421052631578945%\n",
      "\n",
      "batch_num: 48.94736842105264%\n",
      "\n",
      "batch_num: 49.473684210526315%\n",
      "\n",
      "batch_num: 50.0%\n",
      "\n",
      "batch_num: 50.526315789473685%\n",
      "\n",
      "batch_num: 51.05263157894737%\n",
      "\n",
      "batch_num: 51.578947368421055%\n",
      "\n",
      "batch_num: 52.10526315789473%\n",
      "\n",
      "batch_num: 52.63157894736842%\n",
      "\n",
      "batch_num: 53.1578947368421%\n",
      "\n",
      "batch_num: 53.68421052631579%\n",
      "\n",
      "batch_num: 54.21052631578947%\n",
      "\n",
      "batch_num: 54.736842105263165%\n",
      "\n",
      "batch_num: 55.26315789473685%\n",
      "\n",
      "batch_num: 55.78947368421052%\n",
      "\n",
      "batch_num: 56.315789473684205%\n",
      "\n",
      "batch_num: 56.84210526315789%\n",
      "\n",
      "batch_num: 57.36842105263158%\n",
      "\n",
      "batch_num: 57.89473684210527%\n",
      "\n",
      "batch_num: 58.42105263157895%\n",
      "\n",
      "batch_num: 58.94736842105262%\n",
      "\n",
      "batch_num: 59.473684210526315%\n",
      "\n",
      "batch_num: 60.0%\n",
      "\n",
      "batch_num: 60.526315789473685%\n",
      "\n",
      "batch_num: 61.05263157894737%\n",
      "\n",
      "batch_num: 61.578947368421055%\n",
      "\n",
      "batch_num: 62.10526315789474%\n",
      "\n",
      "batch_num: 62.63157894736842%\n",
      "\n",
      "batch_num: 63.1578947368421%\n",
      "\n",
      "batch_num: 63.68421052631579%\n",
      "\n",
      "batch_num: 64.21052631578948%\n",
      "\n",
      "batch_num: 64.73684210526316%\n",
      "\n",
      "batch_num: 65.26315789473685%\n",
      "\n",
      "batch_num: 65.78947368421053%\n",
      "\n",
      "batch_num: 66.3157894736842%\n",
      "\n",
      "batch_num: 66.84210526315789%\n",
      "\n",
      "batch_num: 67.36842105263158%\n",
      "\n",
      "batch_num: 67.89473684210526%\n",
      "\n",
      "batch_num: 68.42105263157895%\n",
      "\n",
      "batch_num: 68.94736842105263%\n",
      "\n",
      "batch_num: 69.47368421052632%\n",
      "\n",
      "batch_num: 70.0%\n",
      "\n",
      "batch_num: 70.52631578947368%\n",
      "\n",
      "batch_num: 71.05263157894737%\n",
      "\n",
      "batch_num: 71.57894736842105%\n",
      "\n",
      "batch_num: 72.10526315789474%\n",
      "\n",
      "batch_num: 72.63157894736842%\n",
      "\n",
      "batch_num: 73.15789473684211%\n",
      "\n",
      "batch_num: 73.68421052631578%\n",
      "\n",
      "batch_num: 74.21052631578947%\n",
      "\n",
      "batch_num: 74.73684210526315%\n",
      "\n",
      "batch_num: 75.26315789473685%\n",
      "\n",
      "batch_num: 75.78947368421053%\n",
      "\n",
      "batch_num: 76.31578947368422%\n",
      "\n",
      "batch_num: 76.84210526315789%\n",
      "\n",
      "batch_num: 77.36842105263158%\n",
      "\n",
      "batch_num: 77.89473684210526%\n",
      "\n",
      "batch_num: 78.42105263157895%\n",
      "\n",
      "batch_num: 78.94736842105263%\n",
      "\n",
      "batch_num: 79.47368421052632%\n",
      "\n",
      "batch_num: 80.0%\n",
      "\n",
      "batch_num: 80.52631578947368%\n",
      "\n",
      "batch_num: 81.05263157894737%\n",
      "\n",
      "batch_num: 81.57894736842105%\n",
      "\n",
      "batch_num: 82.10526315789474%\n",
      "\n",
      "batch_num: 82.63157894736842%\n",
      "\n",
      "batch_num: 83.15789473684211%\n",
      "\n",
      "batch_num: 83.6842105263158%\n",
      "\n",
      "batch_num: 84.21052631578947%\n",
      "\n",
      "batch_num: 84.73684210526315%\n",
      "\n",
      "batch_num: 85.26315789473684%\n",
      "\n",
      "batch_num: 85.78947368421052%\n",
      "\n",
      "batch_num: 86.31578947368422%\n",
      "\n",
      "batch_num: 86.8421052631579%\n",
      "\n",
      "batch_num: 87.36842105263159%\n",
      "\n",
      "batch_num: 87.89473684210526%\n",
      "\n",
      "batch_num: 88.42105263157895%\n",
      "\n",
      "batch_num: 88.94736842105263%\n",
      "\n",
      "batch_num: 89.47368421052632%\n",
      "\n",
      "batch_num: 90.0%\n",
      "\n",
      "batch_num: 90.52631578947368%\n",
      "\n",
      "batch_num: 91.05263157894737%\n",
      "\n",
      "batch_num: 91.57894736842105%\n",
      "\n",
      "batch_num: 92.10526315789474%\n",
      "\n",
      "batch_num: 92.63157894736842%\n",
      "\n",
      "batch_num: 93.15789473684211%\n",
      "\n",
      "batch_num: 93.6842105263158%\n",
      "\n",
      "batch_num: 94.21052631578948%\n",
      "\n",
      "batch_num: 94.73684210526315%\n",
      "\n",
      "batch_num: 95.26315789473684%\n",
      "\n",
      "batch_num: 95.78947368421052%\n",
      "\n",
      "batch_num: 96.3157894736842%\n",
      "\n",
      "batch_num: 96.84210526315789%\n",
      "\n",
      "batch_num: 97.36842105263158%\n",
      "\n",
      "batch_num: 97.89473684210527%\n",
      "\n",
      "batch_num: 98.42105263157895%\n",
      "\n",
      "batch_num: 98.94736842105263%\n",
      "\n",
      "batch_num: 99.47368421052632%\n",
      "\n",
      "Epoch 6 | train loss: 0.0018603381935176195 | val loss: 0.08248889538597061 | accuracy: 97.7%\n",
      "epoch: 7\n",
      "\n",
      "trained: 7\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-15-c5aa7d387409>:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(image, dtype=torch.float), torch.tensor(label_gender, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_num: 0.0%\n",
      "\n",
      "batch_num: 0.5263157894736842%\n",
      "\n",
      "batch_num: 1.0526315789473684%\n",
      "\n",
      "batch_num: 1.5789473684210527%\n",
      "\n",
      "batch_num: 2.1052631578947367%\n",
      "\n",
      "batch_num: 2.631578947368421%\n",
      "\n",
      "batch_num: 3.1578947368421053%\n",
      "\n",
      "batch_num: 3.684210526315789%\n",
      "\n",
      "batch_num: 4.2105263157894735%\n",
      "\n",
      "batch_num: 4.736842105263158%\n",
      "\n",
      "batch_num: 5.263157894736842%\n",
      "\n",
      "batch_num: 5.7894736842105265%\n",
      "\n",
      "batch_num: 6.315789473684211%\n",
      "\n",
      "batch_num: 6.842105263157896%\n",
      "\n",
      "batch_num: 7.368421052631578%\n",
      "\n",
      "batch_num: 7.894736842105263%\n",
      "\n",
      "batch_num: 8.421052631578947%\n",
      "\n",
      "batch_num: 8.947368421052632%\n",
      "\n",
      "batch_num: 9.473684210526317%\n",
      "\n",
      "batch_num: 10.0%\n",
      "\n",
      "batch_num: 10.526315789473683%\n",
      "\n",
      "batch_num: 11.052631578947368%\n",
      "\n",
      "batch_num: 11.578947368421053%\n",
      "\n",
      "batch_num: 12.105263157894736%\n",
      "\n",
      "batch_num: 12.631578947368421%\n",
      "\n",
      "batch_num: 13.157894736842104%\n",
      "\n",
      "batch_num: 13.684210526315791%\n",
      "\n",
      "batch_num: 14.210526315789473%\n",
      "\n",
      "batch_num: 14.736842105263156%\n",
      "\n",
      "batch_num: 15.263157894736842%\n",
      "\n",
      "batch_num: 15.789473684210526%\n",
      "\n",
      "batch_num: 16.315789473684212%\n",
      "\n",
      "batch_num: 16.842105263157894%\n",
      "\n",
      "batch_num: 17.36842105263158%\n",
      "\n",
      "batch_num: 17.894736842105264%\n",
      "\n",
      "batch_num: 18.421052631578945%\n",
      "\n",
      "batch_num: 18.947368421052634%\n",
      "\n",
      "batch_num: 19.473684210526315%\n",
      "\n",
      "batch_num: 20.0%\n",
      "\n",
      "batch_num: 20.526315789473685%\n",
      "\n",
      "batch_num: 21.052631578947366%\n",
      "\n",
      "batch_num: 21.578947368421055%\n",
      "\n",
      "batch_num: 22.105263157894736%\n",
      "\n",
      "batch_num: 22.63157894736842%\n",
      "\n",
      "batch_num: 23.157894736842106%\n",
      "\n",
      "batch_num: 23.684210526315788%\n",
      "\n",
      "batch_num: 24.210526315789473%\n",
      "\n",
      "batch_num: 24.736842105263158%\n",
      "\n",
      "batch_num: 25.263157894736842%\n",
      "\n",
      "batch_num: 25.789473684210527%\n",
      "\n",
      "batch_num: 26.31578947368421%\n",
      "\n",
      "batch_num: 26.842105263157894%\n",
      "\n",
      "batch_num: 27.368421052631582%\n",
      "\n",
      "batch_num: 27.89473684210526%\n",
      "\n",
      "batch_num: 28.421052631578945%\n",
      "\n",
      "batch_num: 28.947368421052634%\n",
      "\n",
      "batch_num: 29.47368421052631%\n",
      "\n",
      "batch_num: 30.0%\n",
      "\n",
      "batch_num: 30.526315789473685%\n",
      "\n",
      "batch_num: 31.05263157894737%\n",
      "\n",
      "batch_num: 31.57894736842105%\n",
      "\n",
      "batch_num: 32.10526315789474%\n",
      "\n",
      "batch_num: 32.631578947368425%\n",
      "\n",
      "batch_num: 33.1578947368421%\n",
      "\n",
      "batch_num: 33.68421052631579%\n",
      "\n",
      "batch_num: 34.21052631578947%\n",
      "\n",
      "batch_num: 34.73684210526316%\n",
      "\n",
      "batch_num: 35.26315789473684%\n",
      "\n",
      "batch_num: 35.78947368421053%\n",
      "\n",
      "batch_num: 36.31578947368421%\n",
      "\n",
      "batch_num: 36.84210526315789%\n",
      "\n",
      "batch_num: 37.368421052631575%\n",
      "\n",
      "batch_num: 37.89473684210527%\n",
      "\n",
      "batch_num: 38.421052631578945%\n",
      "\n",
      "batch_num: 38.94736842105263%\n",
      "\n",
      "batch_num: 39.473684210526315%\n",
      "\n",
      "batch_num: 40.0%\n",
      "\n",
      "batch_num: 40.526315789473685%\n",
      "\n",
      "batch_num: 41.05263157894737%\n",
      "\n",
      "batch_num: 41.578947368421055%\n",
      "\n",
      "batch_num: 42.10526315789473%\n",
      "\n",
      "batch_num: 42.63157894736842%\n",
      "\n",
      "batch_num: 43.15789473684211%\n",
      "\n",
      "batch_num: 43.684210526315795%\n",
      "\n",
      "batch_num: 44.21052631578947%\n",
      "\n",
      "batch_num: 44.73684210526316%\n",
      "\n",
      "batch_num: 45.26315789473684%\n",
      "\n",
      "batch_num: 45.78947368421053%\n",
      "\n",
      "batch_num: 46.31578947368421%\n",
      "\n",
      "batch_num: 46.8421052631579%\n",
      "\n",
      "batch_num: 47.368421052631575%\n",
      "\n",
      "batch_num: 47.89473684210526%\n",
      "\n",
      "batch_num: 48.421052631578945%\n",
      "\n",
      "batch_num: 48.94736842105264%\n",
      "\n",
      "batch_num: 49.473684210526315%\n",
      "\n",
      "batch_num: 50.0%\n",
      "\n",
      "batch_num: 50.526315789473685%\n",
      "\n",
      "batch_num: 51.05263157894737%\n",
      "\n",
      "batch_num: 51.578947368421055%\n",
      "\n",
      "batch_num: 52.10526315789473%\n",
      "\n",
      "batch_num: 52.63157894736842%\n",
      "\n",
      "batch_num: 53.1578947368421%\n",
      "\n",
      "batch_num: 53.68421052631579%\n",
      "\n",
      "batch_num: 54.21052631578947%\n",
      "\n",
      "batch_num: 54.736842105263165%\n",
      "\n",
      "batch_num: 55.26315789473685%\n",
      "\n",
      "batch_num: 55.78947368421052%\n",
      "\n",
      "batch_num: 56.315789473684205%\n",
      "\n",
      "batch_num: 56.84210526315789%\n",
      "\n",
      "batch_num: 57.36842105263158%\n",
      "\n",
      "batch_num: 57.89473684210527%\n",
      "\n",
      "batch_num: 58.42105263157895%\n",
      "\n",
      "batch_num: 58.94736842105262%\n",
      "\n",
      "batch_num: 59.473684210526315%\n",
      "\n",
      "batch_num: 60.0%\n",
      "\n",
      "batch_num: 60.526315789473685%\n",
      "\n",
      "batch_num: 61.05263157894737%\n",
      "\n",
      "batch_num: 61.578947368421055%\n",
      "\n",
      "batch_num: 62.10526315789474%\n",
      "\n",
      "batch_num: 62.63157894736842%\n",
      "\n",
      "batch_num: 63.1578947368421%\n",
      "\n",
      "batch_num: 63.68421052631579%\n",
      "\n",
      "batch_num: 64.21052631578948%\n",
      "\n",
      "batch_num: 64.73684210526316%\n",
      "\n",
      "batch_num: 65.26315789473685%\n",
      "\n",
      "batch_num: 65.78947368421053%\n",
      "\n",
      "batch_num: 66.3157894736842%\n",
      "\n",
      "batch_num: 66.84210526315789%\n",
      "\n",
      "batch_num: 67.36842105263158%\n",
      "\n",
      "batch_num: 67.89473684210526%\n",
      "\n",
      "batch_num: 68.42105263157895%\n",
      "\n",
      "batch_num: 68.94736842105263%\n",
      "\n",
      "batch_num: 69.47368421052632%\n",
      "\n",
      "batch_num: 70.0%\n",
      "\n",
      "batch_num: 70.52631578947368%\n",
      "\n",
      "batch_num: 71.05263157894737%\n",
      "\n",
      "batch_num: 71.57894736842105%\n",
      "\n",
      "batch_num: 72.10526315789474%\n",
      "\n",
      "batch_num: 72.63157894736842%\n",
      "\n",
      "batch_num: 73.15789473684211%\n",
      "\n",
      "batch_num: 73.68421052631578%\n",
      "\n",
      "batch_num: 74.21052631578947%\n",
      "\n",
      "batch_num: 74.73684210526315%\n",
      "\n",
      "batch_num: 75.26315789473685%\n",
      "\n",
      "batch_num: 75.78947368421053%\n",
      "\n",
      "batch_num: 76.31578947368422%\n",
      "\n",
      "batch_num: 76.84210526315789%\n",
      "\n",
      "batch_num: 77.36842105263158%\n",
      "\n",
      "batch_num: 77.89473684210526%\n",
      "\n",
      "batch_num: 78.42105263157895%\n",
      "\n",
      "batch_num: 78.94736842105263%\n",
      "\n",
      "batch_num: 79.47368421052632%\n",
      "\n",
      "batch_num: 80.0%\n",
      "\n",
      "batch_num: 80.52631578947368%\n",
      "\n",
      "batch_num: 81.05263157894737%\n",
      "\n",
      "batch_num: 81.57894736842105%\n",
      "\n",
      "batch_num: 82.10526315789474%\n",
      "\n",
      "batch_num: 82.63157894736842%\n",
      "\n",
      "batch_num: 83.15789473684211%\n",
      "\n",
      "batch_num: 83.6842105263158%\n",
      "\n",
      "batch_num: 84.21052631578947%\n",
      "\n",
      "batch_num: 84.73684210526315%\n",
      "\n",
      "batch_num: 85.26315789473684%\n",
      "\n",
      "batch_num: 85.78947368421052%\n",
      "\n",
      "batch_num: 86.31578947368422%\n",
      "\n",
      "batch_num: 86.8421052631579%\n",
      "\n",
      "batch_num: 87.36842105263159%\n",
      "\n",
      "batch_num: 87.89473684210526%\n",
      "\n",
      "batch_num: 88.42105263157895%\n",
      "\n",
      "batch_num: 88.94736842105263%\n",
      "\n",
      "batch_num: 89.47368421052632%\n",
      "\n",
      "batch_num: 90.0%\n",
      "\n",
      "batch_num: 90.52631578947368%\n",
      "\n",
      "batch_num: 91.05263157894737%\n",
      "\n",
      "batch_num: 91.57894736842105%\n",
      "\n",
      "batch_num: 92.10526315789474%\n",
      "\n",
      "batch_num: 92.63157894736842%\n",
      "\n",
      "batch_num: 93.15789473684211%\n",
      "\n",
      "batch_num: 93.6842105263158%\n",
      "\n",
      "batch_num: 94.21052631578948%\n",
      "\n",
      "batch_num: 94.73684210526315%\n",
      "\n",
      "batch_num: 95.26315789473684%\n",
      "\n",
      "batch_num: 95.78947368421052%\n",
      "\n",
      "batch_num: 96.3157894736842%\n",
      "\n",
      "batch_num: 96.84210526315789%\n",
      "\n",
      "batch_num: 97.36842105263158%\n",
      "\n",
      "batch_num: 97.89473684210527%\n",
      "\n",
      "batch_num: 98.42105263157895%\n",
      "\n",
      "batch_num: 98.94736842105263%\n",
      "\n",
      "batch_num: 99.47368421052632%\n",
      "\n",
      "Epoch 7 | train loss: 0.0023367036032907076 | val loss: 0.07523707928008992 | accuracy: 97.99%\n",
      "epoch: 8\n",
      "\n",
      "trained: 8\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-15-c5aa7d387409>:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(image, dtype=torch.float), torch.tensor(label_gender, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_num: 0.0%\n",
      "\n",
      "batch_num: 0.5263157894736842%\n",
      "\n",
      "batch_num: 1.0526315789473684%\n",
      "\n",
      "batch_num: 1.5789473684210527%\n",
      "\n",
      "batch_num: 2.1052631578947367%\n",
      "\n",
      "batch_num: 2.631578947368421%\n",
      "\n",
      "batch_num: 3.1578947368421053%\n",
      "\n",
      "batch_num: 3.684210526315789%\n",
      "\n",
      "batch_num: 4.2105263157894735%\n",
      "\n",
      "batch_num: 4.736842105263158%\n",
      "\n",
      "batch_num: 5.263157894736842%\n",
      "\n",
      "batch_num: 5.7894736842105265%\n",
      "\n",
      "batch_num: 6.315789473684211%\n",
      "\n",
      "batch_num: 6.842105263157896%\n",
      "\n",
      "batch_num: 7.368421052631578%\n",
      "\n",
      "batch_num: 7.894736842105263%\n",
      "\n",
      "batch_num: 8.421052631578947%\n",
      "\n",
      "batch_num: 8.947368421052632%\n",
      "\n",
      "batch_num: 9.473684210526317%\n",
      "\n",
      "batch_num: 10.0%\n",
      "\n",
      "batch_num: 10.526315789473683%\n",
      "\n",
      "batch_num: 11.052631578947368%\n",
      "\n",
      "batch_num: 11.578947368421053%\n",
      "\n",
      "batch_num: 12.105263157894736%\n",
      "\n",
      "batch_num: 12.631578947368421%\n",
      "\n",
      "batch_num: 13.157894736842104%\n",
      "\n",
      "batch_num: 13.684210526315791%\n",
      "\n",
      "batch_num: 14.210526315789473%\n",
      "\n",
      "batch_num: 14.736842105263156%\n",
      "\n",
      "batch_num: 15.263157894736842%\n",
      "\n",
      "batch_num: 15.789473684210526%\n",
      "\n",
      "batch_num: 16.315789473684212%\n",
      "\n",
      "batch_num: 16.842105263157894%\n",
      "\n",
      "batch_num: 17.36842105263158%\n",
      "\n",
      "batch_num: 17.894736842105264%\n",
      "\n",
      "batch_num: 18.421052631578945%\n",
      "\n",
      "batch_num: 18.947368421052634%\n",
      "\n",
      "batch_num: 19.473684210526315%\n",
      "\n",
      "batch_num: 20.0%\n",
      "\n",
      "batch_num: 20.526315789473685%\n",
      "\n",
      "batch_num: 21.052631578947366%\n",
      "\n",
      "batch_num: 21.578947368421055%\n",
      "\n",
      "batch_num: 22.105263157894736%\n",
      "\n",
      "batch_num: 22.63157894736842%\n",
      "\n",
      "batch_num: 23.157894736842106%\n",
      "\n",
      "batch_num: 23.684210526315788%\n",
      "\n",
      "batch_num: 24.210526315789473%\n",
      "\n",
      "batch_num: 24.736842105263158%\n",
      "\n",
      "batch_num: 25.263157894736842%\n",
      "\n",
      "batch_num: 25.789473684210527%\n",
      "\n",
      "batch_num: 26.31578947368421%\n",
      "\n",
      "batch_num: 26.842105263157894%\n",
      "\n",
      "batch_num: 27.368421052631582%\n",
      "\n",
      "batch_num: 27.89473684210526%\n",
      "\n",
      "batch_num: 28.421052631578945%\n",
      "\n",
      "batch_num: 28.947368421052634%\n",
      "\n",
      "batch_num: 29.47368421052631%\n",
      "\n",
      "batch_num: 30.0%\n",
      "\n",
      "batch_num: 30.526315789473685%\n",
      "\n",
      "batch_num: 31.05263157894737%\n",
      "\n",
      "batch_num: 31.57894736842105%\n",
      "\n",
      "batch_num: 32.10526315789474%\n",
      "\n",
      "batch_num: 32.631578947368425%\n",
      "\n",
      "batch_num: 33.1578947368421%\n",
      "\n",
      "batch_num: 33.68421052631579%\n",
      "\n",
      "batch_num: 34.21052631578947%\n",
      "\n",
      "batch_num: 34.73684210526316%\n",
      "\n",
      "batch_num: 35.26315789473684%\n",
      "\n",
      "batch_num: 35.78947368421053%\n",
      "\n",
      "batch_num: 36.31578947368421%\n",
      "\n",
      "batch_num: 36.84210526315789%\n",
      "\n",
      "batch_num: 37.368421052631575%\n",
      "\n",
      "batch_num: 37.89473684210527%\n",
      "\n",
      "batch_num: 38.421052631578945%\n",
      "\n",
      "batch_num: 38.94736842105263%\n",
      "\n",
      "batch_num: 39.473684210526315%\n",
      "\n",
      "batch_num: 40.0%\n",
      "\n",
      "batch_num: 40.526315789473685%\n",
      "\n",
      "batch_num: 41.05263157894737%\n",
      "\n",
      "batch_num: 41.578947368421055%\n",
      "\n",
      "batch_num: 42.10526315789473%\n",
      "\n",
      "batch_num: 42.63157894736842%\n",
      "\n",
      "batch_num: 43.15789473684211%\n",
      "\n",
      "batch_num: 43.684210526315795%\n",
      "\n",
      "batch_num: 44.21052631578947%\n",
      "\n",
      "batch_num: 44.73684210526316%\n",
      "\n",
      "batch_num: 45.26315789473684%\n",
      "\n",
      "batch_num: 45.78947368421053%\n",
      "\n",
      "batch_num: 46.31578947368421%\n",
      "\n",
      "batch_num: 46.8421052631579%\n",
      "\n",
      "batch_num: 47.368421052631575%\n",
      "\n",
      "batch_num: 47.89473684210526%\n",
      "\n",
      "batch_num: 48.421052631578945%\n",
      "\n",
      "batch_num: 48.94736842105264%\n",
      "\n",
      "batch_num: 49.473684210526315%\n",
      "\n",
      "batch_num: 50.0%\n",
      "\n",
      "batch_num: 50.526315789473685%\n",
      "\n",
      "batch_num: 51.05263157894737%\n",
      "\n",
      "batch_num: 51.578947368421055%\n",
      "\n",
      "batch_num: 52.10526315789473%\n",
      "\n",
      "batch_num: 52.63157894736842%\n",
      "\n",
      "batch_num: 53.1578947368421%\n",
      "\n",
      "batch_num: 53.68421052631579%\n",
      "\n",
      "batch_num: 54.21052631578947%\n",
      "\n",
      "batch_num: 54.736842105263165%\n",
      "\n",
      "batch_num: 55.26315789473685%\n",
      "\n",
      "batch_num: 55.78947368421052%\n",
      "\n",
      "batch_num: 56.315789473684205%\n",
      "\n",
      "batch_num: 56.84210526315789%\n",
      "\n",
      "batch_num: 57.36842105263158%\n",
      "\n",
      "batch_num: 57.89473684210527%\n",
      "\n",
      "batch_num: 58.42105263157895%\n",
      "\n",
      "batch_num: 58.94736842105262%\n",
      "\n",
      "batch_num: 59.473684210526315%\n",
      "\n",
      "batch_num: 60.0%\n",
      "\n",
      "batch_num: 60.526315789473685%\n",
      "\n",
      "batch_num: 61.05263157894737%\n",
      "\n",
      "batch_num: 61.578947368421055%\n",
      "\n",
      "batch_num: 62.10526315789474%\n",
      "\n",
      "batch_num: 62.63157894736842%\n",
      "\n",
      "batch_num: 63.1578947368421%\n",
      "\n",
      "batch_num: 63.68421052631579%\n",
      "\n",
      "batch_num: 64.21052631578948%\n",
      "\n",
      "batch_num: 64.73684210526316%\n",
      "\n",
      "batch_num: 65.26315789473685%\n",
      "\n",
      "batch_num: 65.78947368421053%\n",
      "\n",
      "batch_num: 66.3157894736842%\n",
      "\n",
      "batch_num: 66.84210526315789%\n",
      "\n",
      "batch_num: 67.36842105263158%\n",
      "\n",
      "batch_num: 67.89473684210526%\n",
      "\n",
      "batch_num: 68.42105263157895%\n",
      "\n",
      "batch_num: 68.94736842105263%\n",
      "\n",
      "batch_num: 69.47368421052632%\n",
      "\n",
      "batch_num: 70.0%\n",
      "\n",
      "batch_num: 70.52631578947368%\n",
      "\n",
      "batch_num: 71.05263157894737%\n",
      "\n",
      "batch_num: 71.57894736842105%\n",
      "\n",
      "batch_num: 72.10526315789474%\n",
      "\n",
      "batch_num: 72.63157894736842%\n",
      "\n",
      "batch_num: 73.15789473684211%\n",
      "\n",
      "batch_num: 73.68421052631578%\n",
      "\n",
      "batch_num: 74.21052631578947%\n",
      "\n",
      "batch_num: 74.73684210526315%\n",
      "\n",
      "batch_num: 75.26315789473685%\n",
      "\n",
      "batch_num: 75.78947368421053%\n",
      "\n",
      "batch_num: 76.31578947368422%\n",
      "\n",
      "batch_num: 76.84210526315789%\n",
      "\n",
      "batch_num: 77.36842105263158%\n",
      "\n",
      "batch_num: 77.89473684210526%\n",
      "\n",
      "batch_num: 78.42105263157895%\n",
      "\n",
      "batch_num: 78.94736842105263%\n",
      "\n",
      "batch_num: 79.47368421052632%\n",
      "\n",
      "batch_num: 80.0%\n",
      "\n",
      "batch_num: 80.52631578947368%\n",
      "\n",
      "batch_num: 81.05263157894737%\n",
      "\n",
      "batch_num: 81.57894736842105%\n",
      "\n",
      "batch_num: 82.10526315789474%\n",
      "\n",
      "batch_num: 82.63157894736842%\n",
      "\n",
      "batch_num: 83.15789473684211%\n",
      "\n",
      "batch_num: 83.6842105263158%\n",
      "\n",
      "batch_num: 84.21052631578947%\n",
      "\n",
      "batch_num: 84.73684210526315%\n",
      "\n",
      "batch_num: 85.26315789473684%\n",
      "\n",
      "batch_num: 85.78947368421052%\n",
      "\n",
      "batch_num: 86.31578947368422%\n",
      "\n",
      "batch_num: 86.8421052631579%\n",
      "\n",
      "batch_num: 87.36842105263159%\n",
      "\n",
      "batch_num: 87.89473684210526%\n",
      "\n",
      "batch_num: 88.42105263157895%\n",
      "\n",
      "batch_num: 88.94736842105263%\n",
      "\n",
      "batch_num: 89.47368421052632%\n",
      "\n",
      "batch_num: 90.0%\n",
      "\n",
      "batch_num: 90.52631578947368%\n",
      "\n",
      "batch_num: 91.05263157894737%\n",
      "\n",
      "batch_num: 91.57894736842105%\n",
      "\n",
      "batch_num: 92.10526315789474%\n",
      "\n",
      "batch_num: 92.63157894736842%\n",
      "\n",
      "batch_num: 93.15789473684211%\n",
      "\n",
      "batch_num: 93.6842105263158%\n",
      "\n",
      "batch_num: 94.21052631578948%\n",
      "\n",
      "batch_num: 94.73684210526315%\n",
      "\n",
      "batch_num: 95.26315789473684%\n",
      "\n",
      "batch_num: 95.78947368421052%\n",
      "\n",
      "batch_num: 96.3157894736842%\n",
      "\n",
      "batch_num: 96.84210526315789%\n",
      "\n",
      "batch_num: 97.36842105263158%\n",
      "\n",
      "batch_num: 97.89473684210527%\n",
      "\n",
      "batch_num: 98.42105263157895%\n",
      "\n",
      "batch_num: 98.94736842105263%\n",
      "\n",
      "batch_num: 99.47368421052632%\n",
      "\n",
      "Epoch 8 | train loss: 0.0022711397219130672 | val loss: 0.07989663690519096 | accuracy: 97.77%\n",
      "epoch: 9\n",
      "\n",
      "trained: 9\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-15-c5aa7d387409>:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(image, dtype=torch.float), torch.tensor(label_gender, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_num: 0.0%\n",
      "\n",
      "batch_num: 0.5263157894736842%\n",
      "\n",
      "batch_num: 1.0526315789473684%\n",
      "\n",
      "batch_num: 1.5789473684210527%\n",
      "\n",
      "batch_num: 2.1052631578947367%\n",
      "\n",
      "batch_num: 2.631578947368421%\n",
      "\n",
      "batch_num: 3.1578947368421053%\n",
      "\n",
      "batch_num: 3.684210526315789%\n",
      "\n",
      "batch_num: 4.2105263157894735%\n",
      "\n",
      "batch_num: 4.736842105263158%\n",
      "\n",
      "batch_num: 5.263157894736842%\n",
      "\n",
      "batch_num: 5.7894736842105265%\n",
      "\n",
      "batch_num: 6.315789473684211%\n",
      "\n",
      "batch_num: 6.842105263157896%\n",
      "\n",
      "batch_num: 7.368421052631578%\n",
      "\n",
      "batch_num: 7.894736842105263%\n",
      "\n",
      "batch_num: 8.421052631578947%\n",
      "\n",
      "batch_num: 8.947368421052632%\n",
      "\n",
      "batch_num: 9.473684210526317%\n",
      "\n",
      "batch_num: 10.0%\n",
      "\n",
      "batch_num: 10.526315789473683%\n",
      "\n",
      "batch_num: 11.052631578947368%\n",
      "\n",
      "batch_num: 11.578947368421053%\n",
      "\n",
      "batch_num: 12.105263157894736%\n",
      "\n",
      "batch_num: 12.631578947368421%\n",
      "\n",
      "batch_num: 13.157894736842104%\n",
      "\n",
      "batch_num: 13.684210526315791%\n",
      "\n",
      "batch_num: 14.210526315789473%\n",
      "\n",
      "batch_num: 14.736842105263156%\n",
      "\n",
      "batch_num: 15.263157894736842%\n",
      "\n",
      "batch_num: 15.789473684210526%\n",
      "\n",
      "batch_num: 16.315789473684212%\n",
      "\n",
      "batch_num: 16.842105263157894%\n",
      "\n",
      "batch_num: 17.36842105263158%\n",
      "\n",
      "batch_num: 17.894736842105264%\n",
      "\n",
      "batch_num: 18.421052631578945%\n",
      "\n",
      "batch_num: 18.947368421052634%\n",
      "\n",
      "batch_num: 19.473684210526315%\n",
      "\n",
      "batch_num: 20.0%\n",
      "\n",
      "batch_num: 20.526315789473685%\n",
      "\n",
      "batch_num: 21.052631578947366%\n",
      "\n",
      "batch_num: 21.578947368421055%\n",
      "\n",
      "batch_num: 22.105263157894736%\n",
      "\n",
      "batch_num: 22.63157894736842%\n",
      "\n",
      "batch_num: 23.157894736842106%\n",
      "\n",
      "batch_num: 23.684210526315788%\n",
      "\n",
      "batch_num: 24.210526315789473%\n",
      "\n",
      "batch_num: 24.736842105263158%\n",
      "\n",
      "batch_num: 25.263157894736842%\n",
      "\n",
      "batch_num: 25.789473684210527%\n",
      "\n",
      "batch_num: 26.31578947368421%\n",
      "\n",
      "batch_num: 26.842105263157894%\n",
      "\n",
      "batch_num: 27.368421052631582%\n",
      "\n",
      "batch_num: 27.89473684210526%\n",
      "\n",
      "batch_num: 28.421052631578945%\n",
      "\n",
      "batch_num: 28.947368421052634%\n",
      "\n",
      "batch_num: 29.47368421052631%\n",
      "\n",
      "batch_num: 30.0%\n",
      "\n",
      "batch_num: 30.526315789473685%\n",
      "\n",
      "batch_num: 31.05263157894737%\n",
      "\n",
      "batch_num: 31.57894736842105%\n",
      "\n",
      "batch_num: 32.10526315789474%\n",
      "\n",
      "batch_num: 32.631578947368425%\n",
      "\n",
      "batch_num: 33.1578947368421%\n",
      "\n",
      "batch_num: 33.68421052631579%\n",
      "\n",
      "batch_num: 34.21052631578947%\n",
      "\n",
      "batch_num: 34.73684210526316%\n",
      "\n",
      "batch_num: 35.26315789473684%\n",
      "\n",
      "batch_num: 35.78947368421053%\n",
      "\n",
      "batch_num: 36.31578947368421%\n",
      "\n",
      "batch_num: 36.84210526315789%\n",
      "\n",
      "batch_num: 37.368421052631575%\n",
      "\n",
      "batch_num: 37.89473684210527%\n",
      "\n",
      "batch_num: 38.421052631578945%\n",
      "\n",
      "batch_num: 38.94736842105263%\n",
      "\n",
      "batch_num: 39.473684210526315%\n",
      "\n",
      "batch_num: 40.0%\n",
      "\n",
      "batch_num: 40.526315789473685%\n",
      "\n",
      "batch_num: 41.05263157894737%\n",
      "\n",
      "batch_num: 41.578947368421055%\n",
      "\n",
      "batch_num: 42.10526315789473%\n",
      "\n",
      "batch_num: 42.63157894736842%\n",
      "\n",
      "batch_num: 43.15789473684211%\n",
      "\n",
      "batch_num: 43.684210526315795%\n",
      "\n",
      "batch_num: 44.21052631578947%\n",
      "\n",
      "batch_num: 44.73684210526316%\n",
      "\n",
      "batch_num: 45.26315789473684%\n",
      "\n",
      "batch_num: 45.78947368421053%\n",
      "\n",
      "batch_num: 46.31578947368421%\n",
      "\n",
      "batch_num: 46.8421052631579%\n",
      "\n",
      "batch_num: 47.368421052631575%\n",
      "\n",
      "batch_num: 47.89473684210526%\n",
      "\n",
      "batch_num: 48.421052631578945%\n",
      "\n",
      "batch_num: 48.94736842105264%\n",
      "\n",
      "batch_num: 49.473684210526315%\n",
      "\n",
      "batch_num: 50.0%\n",
      "\n",
      "batch_num: 50.526315789473685%\n",
      "\n",
      "batch_num: 51.05263157894737%\n",
      "\n",
      "batch_num: 51.578947368421055%\n",
      "\n",
      "batch_num: 52.10526315789473%\n",
      "\n",
      "batch_num: 52.63157894736842%\n",
      "\n",
      "batch_num: 53.1578947368421%\n",
      "\n",
      "batch_num: 53.68421052631579%\n",
      "\n",
      "batch_num: 54.21052631578947%\n",
      "\n",
      "batch_num: 54.736842105263165%\n",
      "\n",
      "batch_num: 55.26315789473685%\n",
      "\n",
      "batch_num: 55.78947368421052%\n",
      "\n",
      "batch_num: 56.315789473684205%\n",
      "\n",
      "batch_num: 56.84210526315789%\n",
      "\n",
      "batch_num: 57.36842105263158%\n",
      "\n",
      "batch_num: 57.89473684210527%\n",
      "\n",
      "batch_num: 58.42105263157895%\n",
      "\n",
      "batch_num: 58.94736842105262%\n",
      "\n",
      "batch_num: 59.473684210526315%\n",
      "\n",
      "batch_num: 60.0%\n",
      "\n",
      "batch_num: 60.526315789473685%\n",
      "\n",
      "batch_num: 61.05263157894737%\n",
      "\n",
      "batch_num: 61.578947368421055%\n",
      "\n",
      "batch_num: 62.10526315789474%\n",
      "\n",
      "batch_num: 62.63157894736842%\n",
      "\n",
      "batch_num: 63.1578947368421%\n",
      "\n",
      "batch_num: 63.68421052631579%\n",
      "\n",
      "batch_num: 64.21052631578948%\n",
      "\n",
      "batch_num: 64.73684210526316%\n",
      "\n",
      "batch_num: 65.26315789473685%\n",
      "\n",
      "batch_num: 65.78947368421053%\n",
      "\n",
      "batch_num: 66.3157894736842%\n",
      "\n",
      "batch_num: 66.84210526315789%\n",
      "\n",
      "batch_num: 67.36842105263158%\n",
      "\n",
      "batch_num: 67.89473684210526%\n",
      "\n",
      "batch_num: 68.42105263157895%\n",
      "\n",
      "batch_num: 68.94736842105263%\n",
      "\n",
      "batch_num: 69.47368421052632%\n",
      "\n",
      "batch_num: 70.0%\n",
      "\n",
      "batch_num: 70.52631578947368%\n",
      "\n",
      "batch_num: 71.05263157894737%\n",
      "\n",
      "batch_num: 71.57894736842105%\n",
      "\n",
      "batch_num: 72.10526315789474%\n",
      "\n",
      "batch_num: 72.63157894736842%\n",
      "\n",
      "batch_num: 73.15789473684211%\n",
      "\n",
      "batch_num: 73.68421052631578%\n",
      "\n",
      "batch_num: 74.21052631578947%\n",
      "\n",
      "batch_num: 74.73684210526315%\n",
      "\n",
      "batch_num: 75.26315789473685%\n",
      "\n",
      "batch_num: 75.78947368421053%\n",
      "\n",
      "batch_num: 76.31578947368422%\n",
      "\n",
      "batch_num: 76.84210526315789%\n",
      "\n",
      "batch_num: 77.36842105263158%\n",
      "\n",
      "batch_num: 77.89473684210526%\n",
      "\n",
      "batch_num: 78.42105263157895%\n",
      "\n",
      "batch_num: 78.94736842105263%\n",
      "\n",
      "batch_num: 79.47368421052632%\n",
      "\n",
      "batch_num: 80.0%\n",
      "\n",
      "batch_num: 80.52631578947368%\n",
      "\n",
      "batch_num: 81.05263157894737%\n",
      "\n",
      "batch_num: 81.57894736842105%\n",
      "\n",
      "batch_num: 82.10526315789474%\n",
      "\n",
      "batch_num: 82.63157894736842%\n",
      "\n",
      "batch_num: 83.15789473684211%\n",
      "\n",
      "batch_num: 83.6842105263158%\n",
      "\n",
      "batch_num: 84.21052631578947%\n",
      "\n",
      "batch_num: 84.73684210526315%\n",
      "\n",
      "batch_num: 85.26315789473684%\n",
      "\n",
      "batch_num: 85.78947368421052%\n",
      "\n",
      "batch_num: 86.31578947368422%\n",
      "\n",
      "batch_num: 86.8421052631579%\n",
      "\n",
      "batch_num: 87.36842105263159%\n",
      "\n",
      "batch_num: 87.89473684210526%\n",
      "\n",
      "batch_num: 88.42105263157895%\n",
      "\n",
      "batch_num: 88.94736842105263%\n",
      "\n",
      "batch_num: 89.47368421052632%\n",
      "\n",
      "batch_num: 90.0%\n",
      "\n",
      "batch_num: 90.52631578947368%\n",
      "\n",
      "batch_num: 91.05263157894737%\n",
      "\n",
      "batch_num: 91.57894736842105%\n",
      "\n",
      "batch_num: 92.10526315789474%\n",
      "\n",
      "batch_num: 92.63157894736842%\n",
      "\n",
      "batch_num: 93.15789473684211%\n",
      "\n",
      "batch_num: 93.6842105263158%\n",
      "\n",
      "batch_num: 94.21052631578948%\n",
      "\n",
      "batch_num: 94.73684210526315%\n",
      "\n",
      "batch_num: 95.26315789473684%\n",
      "\n",
      "batch_num: 95.78947368421052%\n",
      "\n",
      "batch_num: 96.3157894736842%\n",
      "\n",
      "batch_num: 96.84210526315789%\n",
      "\n",
      "batch_num: 97.36842105263158%\n",
      "\n",
      "batch_num: 97.89473684210527%\n",
      "\n",
      "batch_num: 98.42105263157895%\n",
      "\n",
      "batch_num: 98.94736842105263%\n",
      "\n",
      "batch_num: 99.47368421052632%\n",
      "\n",
      "Epoch 9 | train loss: 0.0013014032444181411 | val loss: 0.07574949433116919 | accuracy: 97.77%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-15-c5aa7d387409>:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(image, dtype=torch.float), torch.tensor(label_gender, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST ACCURACY: 94.72% | Val. Accuracy: 97.4% | Val. Loss.: 0.06985765574923293\n",
      "\n",
      "\n",
      "Overall Accuracy: 94.72 p/m nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Cellar/jupyterlab/3.0.12/libexec/lib/python3.9/site-packages/numpy/core/_methods.py:261: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/usr/local/Cellar/jupyterlab/3.0.12/libexec/lib/python3.9/site-packages/numpy/core/_methods.py:253: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'pickle' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-ec3e69ca7c1c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'gender_results_mixup.pkl'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m         \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_stat_fold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'pickle' is not defined"
     ]
    }
   ],
   "source": [
    "if to_predict == 'gender':\n",
    "    all_accuracy_gender = []\n",
    "    all_val_loss_gender = []\n",
    "    all_stat_fold = []\n",
    "    \n",
    "    for fold in range(kfold):\n",
    "        all_stat = defaultdict(list)\n",
    "        \n",
    "        # image paths\n",
    "        train_data = train_fold[fold]['image_path'].copy().reset_index(drop=True).to_list()\n",
    "        test_data  = test_fold[fold]['image_path'].copy().reset_index(drop=True).to_list()\n",
    "    \n",
    "        #get label\n",
    "        train_age_label = train_fold[fold]['age'].copy().reset_index(drop=True).to_list()\n",
    "        train_gender_label = train_fold[fold]['gender'].copy().reset_index(drop=True).to_list()\n",
    "        test_age_label = test_fold[fold]['age'].copy().reset_index(drop=True).to_list()\n",
    "        test_gender_label = test_fold[fold]['gender'].copy().reset_index(drop=True).to_list()\n",
    "    \n",
    "        #create train-validation stratified split\n",
    "        sss = StratifiedShuffleSplit(n_splits=10, random_state=random_seed)\n",
    "    \n",
    "        #split based on age, more balanced for both age and gender\n",
    "        train_idx, val_idx = list(sss.split(train_data, train_gender_label))[0]\n",
    "    \n",
    "        train_idx = list(train_idx)\n",
    "        val_idx = list(val_idx)\n",
    "    \n",
    "        #create dataloader for gender\n",
    "        train_dataset = GenderDataset('', \n",
    "                                          list(np.array(train_data)[train_idx]), \n",
    "                                          list(np.array(train_age_label)[train_idx]),\n",
    "                                          list(np.array(train_gender_label)[train_idx]),\n",
    "                                          p_augment = p_augment)\n",
    "        val_dataset   = GenderDataset('', \n",
    "                                          list(np.array(train_data)[val_idx]), \n",
    "                                          list(np.array(train_age_label)[val_idx]),\n",
    "                                          list(np.array(train_gender_label)[val_idx]),\n",
    "                                          validation=True)\n",
    "        test_dataset = GenderDataset('', \n",
    "                                  test_data, \n",
    "                                  test_age_label,\n",
    "                                  test_gender_label, \n",
    "                                  validation=True)\n",
    "    \n",
    "    \n",
    "        train_loader = DataLoader(train_dataset, batch_size=batchsize, shuffle=True)\n",
    "        val_loader   = DataLoader(val_dataset, batch_size=batchsize, shuffle=False)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=batchsize, shuffle=False)\n",
    "    \n",
    "        val_gender_label = list(np.array(train_gender_label)[val_idx])\n",
    "        val_age_label = list(np.array(train_age_label)[val_idx])\n",
    "    \n",
    "    \n",
    "        model = InceptionResnetV1(\n",
    "                        classify=True,\n",
    "                        pretrained='vggface2',\n",
    "                        num_classes=num_gender_classes)\n",
    "        model = model.to(device)\n",
    "    \n",
    "        #optimizer\n",
    "        optimizer = optim.AdamW(model.parameters(), lr = lr_gender)\n",
    "        scheduler = optim.lr_scheduler.MultiStepLR(optimizer, [5,10])\n",
    "    \n",
    "        #loss\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "                    \n",
    "        best_acc_gender = 0\n",
    "        best_val_loss_gender = 999\n",
    "        print(f'Fold {fold+1}\\n')\n",
    "        for epoch in range(num_epochs):\n",
    "            print(f'epoch: {epoch}\\n')\n",
    "            train_loss_gender = 0\n",
    "            val_loss_gender = 0\n",
    "        \n",
    "            #Training\n",
    "            model.train()\n",
    "            iterat = 0\n",
    "            vsego = len(train_loader)\n",
    "            for batch in train_loader:\n",
    "    \n",
    "                print(f'batch_num: {100*(iterat/vsego)}%\\n')\n",
    "                # Load image batch\n",
    "                batch_data, batch_gender_label = batch\n",
    "                batch_data = batch_data.to(device)\n",
    "                batch_gender_label = batch_gender_label.to(device)\n",
    "                \n",
    "                iterat = iterat + 1\n",
    "                # Clear gradients\n",
    "                optimizer.zero_grad()\n",
    "            \n",
    "                with torch.set_grad_enabled(True):\n",
    "                    \n",
    "                    pred_gender = model(batch_data)\n",
    "                    loss_gender = criterion(pred_gender, batch_gender_label)\n",
    "            \n",
    "                    train_loss_gender += loss_gender.detach().item()\n",
    "                    loss_gender.backward()\n",
    "                    optimizer.step()\n",
    "            \n",
    "            #Validation\n",
    "            model.eval()\n",
    "            all_pred_gender = torch.empty(0).to(device)\n",
    "            for batch in val_loader:\n",
    "            \n",
    "                # Load image batch\n",
    "                batch_data, batch_gender_label = batch\n",
    "                batch_data = batch_data.to(device)\n",
    "                batch_gender_label = batch_gender_label.to(device)\n",
    "                \n",
    "                with torch.set_grad_enabled(False):\n",
    "                \n",
    "                    pred_gender = model(batch_data)\n",
    "                   \n",
    "                    loss_gender = criterion(pred_gender, batch_gender_label)\n",
    "            \n",
    "                    val_loss_gender += loss_gender.detach().item()\n",
    "                \n",
    "                    all_pred_gender = torch.cat((all_pred_gender, \n",
    "                            nn.functional.softmax(pred_gender.detach(),dim=1)), 0)\n",
    "                \n",
    "        \n",
    "            train_loss_gender /= len(train_loader)\n",
    "            val_loss_gender /= len(val_loader)\n",
    "        \n",
    "            all_pred_gender = all_pred_gender.cpu().numpy()\n",
    "            pred_label_gender = list(np.argmax(all_pred_gender,axis=1))\n",
    "       \n",
    "        \n",
    "            acc_gender = accuracy_score(val_gender_label, pred_label_gender)\n",
    " \n",
    "            if val_loss_gender < best_val_loss_gender:\n",
    "                best_acc_gender=acc_gender\n",
    "                best_val_loss_gender=val_loss_gender\n",
    "                torch.save(model.state_dict(), f'model{fold}.pth')\n",
    "            \n",
    "            all_stat['train_loss'].append(train_loss_gender)\n",
    "            all_stat['val_loss'].append(val_loss_gender)\n",
    "            all_stat['val_acc'].append(acc_gender)\n",
    "            \n",
    "            print(f'Epoch {epoch} | train loss: {train_loss_gender} | val loss: {val_loss_gender} | accuracy: {round(acc_gender*100, 2)}%')\n",
    "            scheduler.step()\n",
    "        \n",
    "        #INFERENCE\n",
    "        with torch.no_grad():\n",
    "            model.load_state_dict(torch.load(f'model{fold}.pth'))\n",
    "            model.eval()\n",
    "            test_pred_gender = torch.empty(0).to(device)\n",
    "            for batch in test_loader:\n",
    "            \n",
    "                # Load image batch\n",
    "                batch_data, batch_gender_label = batch\n",
    "                batch_data = batch_data.to(device)\n",
    "                batch_gender_label = batch_gender_label.to(device)\n",
    "            \n",
    "                with torch.set_grad_enabled(False):\n",
    "                \n",
    "                    pred_gender = model(batch_data)\n",
    "               \n",
    "                    test_pred_gender = torch.cat((test_pred_gender, \n",
    "                            nn.functional.softmax(pred_gender.detach(),dim=1)), 0)\n",
    "                \n",
    "            test_pred_gender = test_pred_gender.cpu().numpy()\n",
    "            pred_label_gender = list(np.argmax(test_pred_gender,axis=1))\n",
    "        \n",
    "            acc_gender = accuracy_score(test_gender_label, pred_label_gender)\n",
    "            all_stat['test_acc'].append(acc_gender)\n",
    "            all_stat['conf'].append(confusion_matrix(test_gender_label, pred_label_gender, labels=list(range(num_gender_classes))))\n",
    "            all_stat['conf_norm'].append(confusion_matrix(test_gender_label, pred_label_gender,normalize='true', labels=list(range(num_gender_classes))))\n",
    "            all_stat['test_pred'].append(pred_label_gender)\n",
    "            all_stat['test_target'].append(test_gender_label)\n",
    "        all_accuracy_gender.append(acc_gender)\n",
    "        all_val_loss_gender.append(best_val_loss_gender)\n",
    "        print(f'TEST ACCURACY: {round(acc_gender*100,2)}% | Val. Accuracy: {round(best_acc_gender*100,2)}% | Val. Loss.: {best_val_loss_gender}\\n')\n",
    "        \n",
    "        all_stat_fold.append(all_stat)\n",
    "\n",
    "    all_accuracy_gender = np.array(all_accuracy_gender)\n",
    "    all_val_loss_gender = np.array(all_val_loss_gender)\n",
    "\n",
    "    mean_accuracy_gender = round(all_accuracy_gender.mean()*100, 2)\n",
    "\n",
    "    print(f'\\nOverall Accuracy: {mean_accuracy_gender} p/m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "periodic-pointer",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

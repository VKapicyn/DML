{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "mathematical-mortality",
   "metadata": {},
   "outputs": [],
   "source": [
    "from facenet_pytorch import InceptionResnetV1, fixed_image_standardization\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from PIL import Image\n",
    "import random\n",
    "import torch\n",
    "\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import transforms\n",
    "from torchvision import models as models\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "from torch.backends import cudnn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.compose import ColumnTransformer, make_column_transformer, make_column_selector\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.stats import sem\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import albumentations as A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "every-river",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_dataset/9.jpg\n",
      "test_dataset/12.png\n",
      "test_dataset/13.png\n",
      "test_dataset/11.png\n",
      "test_dataset/10.jpg\n",
      "test_dataset/8.jpeg\n",
      "test_dataset/4.png\n",
      "test_dataset/5.png\n",
      "test_dataset/7.jpg\n",
      "test_dataset/6.png\n",
      "test_dataset/2.png\n",
      "test_dataset/3.png\n",
      "test_dataset/1.png\n",
      "test_dataset/14\n"
     ]
    }
   ],
   "source": [
    "test_images_path = 'test_dataset/'\n",
    "\n",
    "def load_images_from_folder(folder): \n",
    "    images = []\n",
    "    for filename in os.listdir(folder): # читаю все предобратанные картинки с фигурами\n",
    "        print(os.path.join(folder,filename))\n",
    "        images.append(os.path.join(folder,filename))\n",
    "    return images\n",
    "\n",
    "images = load_images_from_folder(test_images_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "related-prophet",
   "metadata": {},
   "outputs": [],
   "source": [
    "genderModel = InceptionResnetV1(\n",
    "    classify=True,\n",
    "    pretrained='vggface2',\n",
    "    num_classes= 2)\n",
    "\n",
    "genderModel.load_state_dict(torch.load('models/gender_model0.pth'))\n",
    "genderModel.eval()\n",
    "\n",
    "ageModel = InceptionResnetV1(\n",
    "    classify=True,\n",
    "    pretrained='vggface2',\n",
    "    num_classes= 4)\n",
    "\n",
    "ageModel.load_state_dict(torch.load('models/age_model0.pth'))\n",
    "ageModel.eval()\n",
    "\n",
    "trans = transforms.Compose([\n",
    "    np.float32,\n",
    "    transforms.ToTensor(),\n",
    "    fixed_image_standardization\n",
    "])\n",
    "\n",
    "class GenderDataset(Dataset):\n",
    "    def __init__(self, path, image_files, labels_age, labels_gender, p_augment=0.5,  validation=False):\n",
    "        self.path = path\n",
    "        self.X = image_files\n",
    "        self.y_age = labels_age\n",
    "        self.y_gender = labels_gender\n",
    "        self.resize = A.Resize(160, 160, always_apply=True)\n",
    "        self.transform = trans\n",
    "        \n",
    "    def __len__(self):\n",
    "        return (len(self.X))\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        image = np.asarray(self.X[i])\n",
    "        image = self.resize(image=image)['image']\n",
    "        image = self.transform(image)\n",
    "        label_age = self.y_age[i]\n",
    "        label_gender = self.y_gender[i]\n",
    "        \n",
    "        return torch.tensor(image, dtype=torch.float), torch.tensor(label_gender, dtype=torch.long)\n",
    "\n",
    "class AgeDataset(Dataset):\n",
    "    def __init__(self, path, image_files, labels_age, labels_gender, p_augment=0.5,  validation=False):\n",
    "        self.path = path\n",
    "        self.X = image_files\n",
    "        self.y_age = labels_age\n",
    "        self.y_gender = labels_gender\n",
    "        self.resize = A.Resize(160, 160, always_apply=True)\n",
    "        self.transform = trans\n",
    "        \n",
    "    def __len__(self):\n",
    "        return (len(self.X))\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        image = np.asarray(self.X[i])\n",
    "        image = self.resize(image=image)['image']\n",
    "        image = self.transform(image)\n",
    "        label_age = self.y_age[i]\n",
    "        label_gender = self.y_gender[i]\n",
    "        \n",
    "        return torch.tensor(image, dtype=torch.float), torch.tensor(label_age, dtype=torch.long)\n",
    "\n",
    "def predict_gender_by_image(image_path):\n",
    "    device = torch.device(\"cpu\")\n",
    "    results = torch.empty(0).to(device)\n",
    "\n",
    "    will_predict = [0]*len(image_path)\n",
    "    dataset = GenderDataset('',\n",
    "                            image_path, \n",
    "                            will_predict,\n",
    "                            will_predict, \n",
    "                            validation=True)\n",
    "\n",
    "    loader = DataLoader(dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "    for batch in loader:\n",
    "        batch_data, batch_label = batch\n",
    "        predict = genderModel(batch_data)\n",
    "\n",
    "        results = torch.cat((results,\n",
    "                             nn.functional.softmax(predict.detach(),dim=1)), 0)\n",
    "\n",
    "        results = results.cpu().numpy()\n",
    "    return results\n",
    "\n",
    "def predict_age_by_image(image_path):\n",
    "    device = torch.device(\"cpu\")\n",
    "    results = torch.empty(0).to(device)\n",
    "\n",
    "    will_predict = [0]*len(image_path)\n",
    "    dataset = AgeDataset('',\n",
    "                            image_path, \n",
    "                            will_predict,\n",
    "                            will_predict, \n",
    "                            validation=True)\n",
    "\n",
    "    loader = DataLoader(dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "    for batch in loader:\n",
    "        batch_data, batch_label = batch\n",
    "        predict = ageModel(batch_data)\n",
    "\n",
    "        results = torch.cat((results,\n",
    "                             nn.functional.softmax(predict.detach(),dim=1)), 0)\n",
    "\n",
    "        results = results.cpu().numpy()\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "tracked-treasury",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "_age_labels = [\n",
    "    'Lichinus 0-6',\n",
    "    'Schegol 8-23',\n",
    "    'Bumer 25-32',\n",
    "    'Starper 35+'\n",
    "]\n",
    "\n",
    "def crop_faces(image_path):\n",
    "    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "    image = Image.open(image_path)\n",
    "    image = image.convert('RGB')\n",
    "\n",
    "    basewidth = 1280\n",
    "    wpercent = basewidth / image.size[0]\n",
    "    hsize = int(image.size[1]*wpercent)\n",
    "    image = image.resize((basewidth,hsize), Image.ANTIALIAS)\n",
    "\n",
    "    image = np.array(image, 'uint8')\n",
    "\n",
    "    faces = face_cascade.detectMultiScale(image, scaleFactor=1.2, minNeighbors=2, minSize=(160, 160))\n",
    "    images = []\n",
    "    for (x,y,w,h) in faces:\n",
    "        _image = image[y:y+h,x:x+w]\n",
    "        images.append(_image)\n",
    "\n",
    "    return images\n",
    "\n",
    "def sub_faces(image_path, predict_gender, predict_age):\n",
    "    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "    image = Image.open(image_path)\n",
    "    image = image.convert('RGB')\n",
    "\n",
    "    basewidth = 1280\n",
    "    wpercent = basewidth / image.size[0]\n",
    "    hsize = int(image.size[1]*wpercent)\n",
    "    image = image.resize((basewidth,hsize), Image.ANTIALIAS)\n",
    "\n",
    "    image = np.array(image, 'uint8')\n",
    "\n",
    "    faces = face_cascade.detectMultiScale(image, scaleFactor=1.2, minNeighbors=2, minSize=(160, 160))\n",
    "    images = []\n",
    "    \n",
    "    i = 0\n",
    "    for (x,y,w,h) in faces:\n",
    "        \n",
    "        gender_label = \"\"\n",
    "        if predict_gender[i][0]>predict_gender[i][1]:\n",
    "            gender_label = \"Woman\"\n",
    "        else:\n",
    "            gender_label = \"Man\"\n",
    "        \n",
    "        age_maximum = max(predict_age[i])\n",
    "        age_max_item = 0\n",
    "        for k, item in enumerate(predict_age[i]):\n",
    "            #print(k, item)\n",
    "            if item == age_maximum:\n",
    "                age_max_item = k\n",
    "                \n",
    "        age_label = f'[{_age_labels[age_max_item]}]'\n",
    "        \n",
    "        image = cv2.rectangle(image,(x,y),(x+w,y+h),(255,0,0),4)\n",
    "        \n",
    "        cv2.putText(image, age_label, \n",
    "            (x,y-15), \n",
    "            cv2.FONT_HERSHEY_SIMPLEX, \n",
    "            2,\n",
    "            (255,255,0),\n",
    "            5)\n",
    "        cv2.putText(image, gender_label, \n",
    "            (x,y+h+35), \n",
    "            cv2.FONT_HERSHEY_SIMPLEX, \n",
    "            2,\n",
    "            (255,255,0),\n",
    "            5)\n",
    "        i += 1\n",
    "\n",
    "    image = Image.fromarray(image)\n",
    "    image.save(f'augment/test.jpg')\n",
    "    #fig, ax = plt.subplots(figsize=(10,10))\n",
    "    #ax.imshow(image)\n",
    "    #plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fuzzy-subsection",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-fb41e8c076d7>:42: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(image, dtype=torch.float), torch.tensor(label_gender, dtype=torch.long)\n",
      "<ipython-input-3-fb41e8c076d7>:63: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(image, dtype=torch.float), torch.tensor(label_age, dtype=torch.long)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "unknown file extension: ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/Cellar/jupyterlab/3.0.12/libexec/lib/python3.9/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, fp, format, **params)\u001b[0m\n\u001b[1;32m   2143\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2144\u001b[0;31m                 \u001b[0mformat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEXTENSION\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mext\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2145\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: ''",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-32d07a5a937f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mpredict_gender\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_gender_by_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfaces\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mpredict_age\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_age_by_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfaces\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0msub_faces\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredict_gender\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredict_age\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Лица не обнаружены\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-d39493f6ef98>\u001b[0m in \u001b[0;36msub_faces\u001b[0;34m(image_path, predict_gender, predict_age)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m     \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'augment/image_path'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m     \u001b[0;31m#fig, ax = plt.subplots(figsize=(10,10))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;31m#ax.imshow(image)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/jupyterlab/3.0.12/libexec/lib/python3.9/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, fp, format, **params)\u001b[0m\n\u001b[1;32m   2144\u001b[0m                 \u001b[0mformat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEXTENSION\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mext\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2145\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2146\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"unknown file extension: {ext}\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2148\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mSAVE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: unknown file extension: "
     ]
    }
   ],
   "source": [
    "for image in images:\n",
    "    target_image = image\n",
    "    faces = crop_faces(target_image)\n",
    "    if len(faces)>0:\n",
    "        predict_gender = predict_gender_by_image(faces)\n",
    "        predict_age = predict_age_by_image(faces)\n",
    "        sub_faces(target_image, predict_gender, predict_age)\n",
    "    else:\n",
    "        print(\"Лица не обнаружены\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affected-logic",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
